---
title: "Analysis and Modelling"
description: "Analyse the data and create models to support the expected outputs."
image: 02-analysis-modelling.jpg
image-alt: Photo by Shubham Dhage on Unsplash
---

In this step, we turn our data into insights. The work done here can vary. It can include small data extraction jobs, creating reports and dashboards, and deploying machine learning models. The analysis or modelling you conduct is also heavily reliant on the outputs you agreed with the customer.

## Conducting the Work

Starting from the base tables, we want to create the agreed outputs. Typically we use R or Python to do this.

- Use Github to handle version control, issues and code reviews.
- In Python, use object-oriented programming (OOP); although R does have support for OOP, it is not a big paradigm like in Python.
- Be careful of premature optimisation (called out by computer scientist Donald Knuth as 'the root of all evil' in software development).
- Start small, such as limiting data to some subset when developing metrics; this makes it easier to spot issues and allows for faster iteration of code.
- Check the data looks correct at each step of transforming it; better still, know what it _should_ be before writing and applying the step (the principle behind test driven development, TDD).
- Work on one component of output at a time, for example using `{shiny}` modules as in our [template](https://github.com/nhsbsa-data-analytics/nhsbsaShinyR).
- Frequently run code as you write it, so any issues are encountered early and their source is clearer.
- The final code should use RAP principles [LINK?].
- If it is appropriate, such as needing to show simple summary statistics, just use Excel.
- R is especially suited to use for data analysis and visualisation, via Rmarkdown and Shiny.
- Python has most widespread support for machine learning and is more widely integrated with cloud based platforms.
- Power BI is a possible alternative to R for creating dashboards, but due to licensing may not be suitable for certain end users.
- You may produce multiple smaller data sets to be used in your outputs; the considerations applying to base tables apply equally to these.
- There are pages on coding with R and Python in the internal DALL Wiki.

## Check the Quality

As with data preparation, it's important to test and validate your results. 

- Where possible, check results against other sources.
- Other sources could be data from previous initiatives, Management Information dashboards or ePACT2.

## Sensitive Data

- Wherever data is saved, it must be in a secure location as per data security policy (LINK?).
- Sensitive data should be removed or obfuscated unless it is absolutely necessary to retain it; bear in mind the data governance policies (LINK?).
- For certain usage of sensitive data, the Caldicott Guardians (LINK?) should be consulted.
