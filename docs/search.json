[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NHSBSA DALL Playbook",
    "section": "",
    "text": "Add Text Here\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBefore problem definition\n\n\nBefore problem definition\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProblem definition\n\n\nProblem definition\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis\n\n\nAnalysis loop - prepare data, analyse it and review until outputs in problem definition are finalised.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrap-up\n\n\nWrap-up\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "guidance/01-pre-pd/index.html",
    "href": "guidance/01-pre-pd/index.html",
    "title": "Before problem definition",
    "section": "",
    "text": "Add text here\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData preparation\n\n\nPrepare data and create base table(s)\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "guidance/01-pre-pd/pre-pd-steps/01-step.html",
    "href": "guidance/01-pre-pd/pre-pd-steps/01-step.html",
    "title": "Data preparation",
    "section": "",
    "text": "The goal of this step is to end up with one or more base tables on which analysis is then conducted. The base tables should include only data relevant, or possibly relevant, to the initiative. Some tables can have hundreds of columns, so reducing the footprint in your base tables is a recommended. Data preparation may be iterative, as the analysis becomes refined with frequent feedback from stakeholders.\nThe tables relevant to the initiative will have been identified in the process of writing the problem definition. Before using them it is important to get an idea of the data quality. This consists of summary statistics including at least\n\nData type\nMost common values for categorical fields, or minima/maxima and median for numeric\nCompleteness\nCross-correlation\n\nThere is the possibility of creating a SQL macro or stored procedure to generate the summary statistics in Oracle itself. This will streamline the process and is reproducible and promotes a consistent treatment of data quality.\n\n\nThis is the data type as stored. This may or may not be the same as what you expect. For example, it is common to use a numeric key in a fact table that represents a categorical value found in a dimension table. See the Wiki for more on fact and dimension tables (TODO).\n\n\n\nFor categorical fields, it is useful to have an idea of the distribution of values. For example, if considering use of machine learning techniques that are sensitive to imbalanced data. Even when considering techniques that handle imbalanced data well keeping the distribution in mind is useful when interpreting and reporting on the the results.\n\n\n\nFor numeric fields, this is the analogue to common values for categorical data. The values will give you an idea of how the data is distributed. You could go further and plot a histogram for better visualisation of this. NOTE: Consider adding to Rmarkdown example\n\n\n\nMany techniques are sensitive to missing data. Extreme cases of this can even prevent any useful analysis, so needs to be mitigated in some way. How it is mitigated will depend on the techniques employed. One thing to look out for is cases where a field has values only after, or before, a certain time. When this occurs it may limit some specific analysis to the time period for which values exist.\n\n\n\nSome techniques are resistant to correlation between explanatory variables, while others require all but one such correlated variable to be removed. In either case it is useful to understand what, if any, correlation between variables exists. The correlations can be computed with R, python or directly in Oracle. See these guides for more info\nR Python Oracle\n\n\n\nThe template initiative Data folder contains a Rmarkdown, data_summary.Rmd, which can be used as an example or starting point for data quality checking. When using it make sure to add notes of the findings for future reference.\nAn alternative would be to do it directly in the database, such as suggested on this github issue.\nIn addition to any Rmarkdown used, you may find it helpful to keep a record of specific counts found during initial EDA and during any further EDA performed while preparing the base tables. An example Excel file is in the template initiative Data folder, EDA figures.xlsx."
  },
  {
    "objectID": "guidance/01-pre-pd/pre-pd-steps/01-step.html#data-cleansing",
    "href": "guidance/01-pre-pd/pre-pd-steps/01-step.html#data-cleansing",
    "title": "Data preparation",
    "section": "Data cleansing",
    "text": "Data cleansing\n\nMissingness\nThe data may have some degree of missingness. How it is handled will depend on the both the data and the techniques to be employed in the analysis. There are many techniques to handle this. Sometimes the best method is obvious, sometimes you will have to choose between several potentially viable methods.\n\n\nData types\nFrom the data summary generated you may find some fields that are better expressed in a different data type.\n\n\nKeep or discard fields\nIn the first iteration it is best to keep any and all fields that may be useful in the analysis. As the analysis is refined through exploration and feedback, you can discard fields no longer deemed relevant. By the end, only fields which are used in the analysis should remain in the base tables. An exception to this is when a future phase of work is planned. In such cases it may make sense to keep some fields not immediately useful available."
  },
  {
    "objectID": "guidance/01-pre-pd/pre-pd-steps/01-step.html#rap-considerations",
    "href": "guidance/01-pre-pd/pre-pd-steps/01-step.html#rap-considerations",
    "title": "Data preparation",
    "section": "RAP considerations",
    "text": "RAP considerations\nDon’t repeat yourself is a well known adage in software development. While we are not writing software, it still applies and is a core principle of RAP. Pull out repeated code into functions; this allows for good tests to be written, another core principle of RAP. Variables should not be spread throughout a script, but instead gathered together into a config file or single block at the top of a script."
  },
  {
    "objectID": "guidance/01-pre-pd/pre-pd-steps/01-step.html#note-keeping",
    "href": "guidance/01-pre-pd/pre-pd-steps/01-step.html#note-keeping",
    "title": "Data preparation",
    "section": "Note keeping",
    "text": "Note keeping\nThroughout data preparation, ensure to make notes (saved in the initiative folder) of any potential or realised issues, caveats to be included in final outputs and decisions made with their reasoning. This will help in writing the supporting text in the main outputs."
  },
  {
    "objectID": "guidance/01-pre-pd/pre-pd-steps/01-step.html#validation",
    "href": "guidance/01-pre-pd/pre-pd-steps/01-step.html#validation",
    "title": "Data preparation",
    "section": "Validation",
    "text": "Validation\nSome thought should be given to how to validate the base tables are a true reflection of what is intended. Some data may be checked against other data sources, such ePACT2. In addition, adding tests is a worthwhile investment. With tests you can be more confident that any future change that introduces an error will be caught quickly.\nIf the data is something that you would expect personal details to appear in, you can also confirm you see what you would expect for yourself."
  },
  {
    "objectID": "guidance/01-pre-pd/pre-pd-steps/01-step.html#miro-points-for-reference-to-be-removed",
    "href": "guidance/01-pre-pd/pre-pd-steps/01-step.html#miro-points-for-reference-to-be-removed",
    "title": "Data preparation",
    "section": "Miro points (for reference, to be removed)",
    "text": "Miro points (for reference, to be removed)\nKeep small spreadsheet to keep track of all EDA results\nAlso links with business understanding\nInvest the time into creating accurate base tables\nEither R or SQL for her analysis, just whatever she feels like\nR markdown is quite good to lift into presentations\nUses data miner to extract insights and aggregate SQL tables with the info\nDALP data is mostly snapshot data, some get automatically refreshed, otherwise we ask for it, get it from DWCP or external system\nCan do SQL script and put all results into Excel\nSQL to do base tables, get all the columns you need and cleaned. Then analysis in R\nCould pull base table into R via DBI or DBPLYR\nKeep detailed notes as you progress - Excel, Rmd, PowerPoint. This will speed up creating your outputs"
  },
  {
    "objectID": "guidance/02-pd/index.html",
    "href": "guidance/02-pd/index.html",
    "title": "Problem definition",
    "section": "",
    "text": "In this section, we aim to write and refine the problem definition until we have fully understood the problem we are trying to solve. The terms of the problem definition are agreed with the customer before moving onto the analysis section."
  },
  {
    "objectID": "guidance/02-pd/pd-steps/01-step.html",
    "href": "guidance/02-pd/pd-steps/01-step.html",
    "title": "Data preparation",
    "section": "",
    "text": "The goal of this step is to end up with one or more base tables on which analysis is then conducted. The base tables should include only data relevant, or possibly relevant, to the initiative. Some tables can have hundreds of columns, so reducing the footprint in your base tables is a recommended. Data preparation may be iterative, as the analysis becomes refined with frequent feedback from stakeholders.\nThe tables relevant to the initiative will have been identified in the process of writing the problem definition. Before using them it is important to get an idea of the data quality. This consists of summary statistics including at least\n\nData type\nMost common values for categorical fields, or minima/maxima and median for numeric\nCompleteness\nCross-correlation\n\nThere is the possibility of creating a SQL macro or stored procedure to generate the summary statistics in Oracle itself. This will streamline the process and is reproducible and promotes a consistent treatment of data quality.\n\n\nThis is the data type as stored. This may or may not be the same as what you expect. For example, it is common to use a numeric key in a fact table that represents a categorical value found in a dimension table. See the Wiki for more on fact and dimension tables (TODO).\n\n\n\nFor categorical fields, it is useful to have an idea of the distribution of values. For example, if considering use of machine learning techniques that are sensitive to imbalanced data. Even when considering techniques that handle imbalanced data well keeping the distribution in mind is useful when interpreting and reporting on the the results.\n\n\n\nFor numeric fields, this is the analogue to common values for categorical data. The values will give you an idea of how the data is distributed. You could go further and plot a histogram for better visualisation of this. NOTE: Consider adding to Rmarkdown example\n\n\n\nMany techniques are sensitive to missing data. Extreme cases of this can even prevent any useful analysis, so needs to be mitigated in some way. How it is mitigated will depend on the techniques employed. One thing to look out for is cases where a field has values only after, or before, a certain time. When this occurs it may limit some specific analysis to the time period for which values exist.\n\n\n\nSome techniques are resistant to correlation between explanatory variables, while others require all but one such correlated variable to be removed. In either case it is useful to understand what, if any, correlation between variables exists. The correlations can be computed with R, python or directly in Oracle. See these guides for more info\nR Python Oracle\n\n\n\nThe template initiative Data folder contains a Rmarkdown, data_summary.Rmd, which can be used as an example or starting point for data quality checking. When using it make sure to add notes of the findings for future reference.\nAn alternative would be to do it directly in the database, such as suggested on this github issue.\nIn addition to any Rmarkdown used, you may find it helpful to keep a record of specific counts found during initial EDA and during any further EDA performed while preparing the base tables. An example Excel file is in the template initiative Data folder, EDA figures.xlsx."
  },
  {
    "objectID": "guidance/02-pd/pd-steps/01-step.html#data-cleansing",
    "href": "guidance/02-pd/pd-steps/01-step.html#data-cleansing",
    "title": "Data preparation",
    "section": "Data cleansing",
    "text": "Data cleansing\n\nMissingness\nThe data may have some degree of missingness. How it is handled will depend on the both the data and the techniques to be employed in the analysis. There are many techniques to handle this. Sometimes the best method is obvious, sometimes you will have to choose between several potentially viable methods.\n\n\nData types\nFrom the data summary generated you may find some fields that are better expressed in a different data type.\n\n\nKeep or discard fields\nIn the first iteration it is best to keep any and all fields that may be useful in the analysis. As the analysis is refined through exploration and feedback, you can discard fields no longer deemed relevant. By the end, only fields which are used in the analysis should remain in the base tables. An exception to this is when a future phase of work is planned. In such cases it may make sense to keep some fields not immediately useful available."
  },
  {
    "objectID": "guidance/02-pd/pd-steps/01-step.html#rap-considerations",
    "href": "guidance/02-pd/pd-steps/01-step.html#rap-considerations",
    "title": "Data preparation",
    "section": "RAP considerations",
    "text": "RAP considerations\nDon’t repeat yourself is a well known adage in software development. While we are not writing software, it still applies and is a core principle of RAP. Pull out repeated code into functions; this allows for good tests to be written, another core principle of RAP. Variables should not be spread throughout a script, but instead gathered together into a config file or single block at the top of a script."
  },
  {
    "objectID": "guidance/02-pd/pd-steps/01-step.html#note-keeping",
    "href": "guidance/02-pd/pd-steps/01-step.html#note-keeping",
    "title": "Data preparation",
    "section": "Note keeping",
    "text": "Note keeping\nThroughout data preparation, ensure to make notes (saved in the initiative folder) of any potential or realised issues, caveats to be included in final outputs and decisions made with their reasoning. This will help in writing the supporting text in the main outputs."
  },
  {
    "objectID": "guidance/02-pd/pd-steps/01-step.html#validation",
    "href": "guidance/02-pd/pd-steps/01-step.html#validation",
    "title": "Data preparation",
    "section": "Validation",
    "text": "Validation\nSome thought should be given to how to validate the base tables are a true reflection of what is intended. Some data may be checked against other data sources, such ePACT2. In addition, adding tests is a worthwhile investment. With tests you can be more confident that any future change that introduces an error will be caught quickly.\nIf the data is something that you would expect personal details to appear in, you can also confirm you see what you would expect for yourself."
  },
  {
    "objectID": "guidance/02-pd/pd-steps/01-step.html#miro-points-for-reference-to-be-removed",
    "href": "guidance/02-pd/pd-steps/01-step.html#miro-points-for-reference-to-be-removed",
    "title": "Data preparation",
    "section": "Miro points (for reference, to be removed)",
    "text": "Miro points (for reference, to be removed)\nKeep small spreadsheet to keep track of all EDA results\nAlso links with business understanding\nInvest the time into creating accurate base tables\nEither R or SQL for her analysis, just whatever she feels like\nR markdown is quite good to lift into presentations\nUses data miner to extract insights and aggregate SQL tables with the info\nDALP data is mostly snapshot data, some get automatically refreshed, otherwise we ask for it, get it from DWCP or external system\nCan do SQL script and put all results into Excel\nSQL to do base tables, get all the columns you need and cleaned. Then analysis in R\nCould pull base table into R via DBI or DBPLYR\nKeep detailed notes as you progress - Excel, Rmd, PowerPoint. This will speed up creating your outputs"
  },
  {
    "objectID": "guidance/03-analysis/analysis-steps/01-data-prep.html",
    "href": "guidance/03-analysis/analysis-steps/01-data-prep.html",
    "title": "Data preparation",
    "section": "",
    "text": "Given you have written your problem definition and the customer has signed off on it, it is time to start preparing your data. The aim of this step is to create one or more base tables that feed into your analysis or modelling. Data preparation may be iterative, as the analysis or modelling in the next step becomes refined with frequent feedback from stakeholders."
  },
  {
    "objectID": "guidance/03-analysis/analysis-steps/01-data-prep.html#data-cleansing",
    "href": "guidance/03-analysis/analysis-steps/01-data-prep.html#data-cleansing",
    "title": "Data preparation",
    "section": "Data cleansing",
    "text": "Data cleansing\n\nMissingness\nThe data may have some degree of missingness. How it is handled will depend on the both the data and the techniques to be employed in the analysis. There are many techniques to handle this. Sometimes the best method is obvious, sometimes you will have to choose between several potentially viable methods.\n\n\nData types\nFrom the data summary generated you may find some fields that are better expressed in a different data type.\n\n\nKeep or discard fields\nIn the first iteration it is best to keep any and all fields that may be useful in the analysis. As the analysis is refined through exploration and feedback, you can discard fields no longer deemed relevant. By the end, only fields which are used in the analysis should remain in the base tables. An exception to this is when a future phase of work is planned. In such cases it may make sense to keep some fields not immediately useful available."
  },
  {
    "objectID": "guidance/03-analysis/analysis-steps/01-data-prep.html#rap-considerations",
    "href": "guidance/03-analysis/analysis-steps/01-data-prep.html#rap-considerations",
    "title": "Data preparation",
    "section": "RAP considerations",
    "text": "RAP considerations\nDon’t repeat yourself is a well known adage in software development. While we are not writing software, it still applies and is a core principle of RAP. Pull out repeated code into functions; this allows for good tests to be written, another core principle of RAP. Variables should not be spread throughout a script, but instead gathered together into a config file or single block at the top of a script."
  },
  {
    "objectID": "guidance/03-analysis/analysis-steps/01-data-prep.html#note-keeping",
    "href": "guidance/03-analysis/analysis-steps/01-data-prep.html#note-keeping",
    "title": "Data preparation",
    "section": "Note keeping",
    "text": "Note keeping\nThroughout data preparation, ensure to make notes (saved in the initiative folder) of any potential or realised issues, caveats to be included in final outputs and decisions made with their reasoning. This will help in writing the supporting text in the main outputs."
  },
  {
    "objectID": "guidance/03-analysis/analysis-steps/01-data-prep.html#validation",
    "href": "guidance/03-analysis/analysis-steps/01-data-prep.html#validation",
    "title": "Data preparation",
    "section": "Validation",
    "text": "Validation\nSome thought should be given to how to validate the base tables are a true reflection of what is intended. Some data may be checked against other data sources, such ePACT2. In addition, adding tests is a worthwhile investment. With tests you can be more confident that any future change that introduces an error will be caught quickly.\nIf the data is something that you would expect personal details to appear in, you can also confirm you see what you would expect for yourself."
  },
  {
    "objectID": "guidance/03-analysis/analysis-steps/02-analysis-modelling.html",
    "href": "guidance/03-analysis/analysis-steps/02-analysis-modelling.html",
    "title": "Analysis and modelling",
    "section": "",
    "text": "In this step, we turn our data into insights. The work done here can vary. It can include small data extraction jobs, creating reports and dashboards, and deploying machine learning models. The analysis or modelling you conduct is also heavily reliant on the outputs you agreed with the customer."
  },
  {
    "objectID": "guidance/03-analysis/analysis-steps/02-analysis-modelling.html#rap-considerations",
    "href": "guidance/03-analysis/analysis-steps/02-analysis-modelling.html#rap-considerations",
    "title": "Analysis and modelling",
    "section": "RAP considerations",
    "text": "RAP considerations\nDon’t repeat yourself is a well known adage in software development. While we are not writing software, it still applies and is a core principle of RAP. Pull out repeated code into functions; this allows for good tests to be written, another core principle of RAP. Variables should not be spread throughout a script, but instead gathered together into a config file or single block at the top of a script."
  },
  {
    "objectID": "guidance/03-analysis/analysis-steps/02-analysis-modelling.html#note-keeping",
    "href": "guidance/03-analysis/analysis-steps/02-analysis-modelling.html#note-keeping",
    "title": "Analysis and modelling",
    "section": "Note keeping",
    "text": "Note keeping\nThroughout the analysis, ensure to make notes (saved in the initiative folder) of any potential or realised issues, caveats to be included in final outputs and decisions made with their reasoning. This will help in writing the supporting text in the main outputs."
  },
  {
    "objectID": "guidance/03-analysis/analysis-steps/02-analysis-modelling.html#validation",
    "href": "guidance/03-analysis/analysis-steps/02-analysis-modelling.html#validation",
    "title": "Analysis and modelling",
    "section": "Validation",
    "text": "Validation\nSome thought should be given to how to validate the results of the analysis. Some data may be checked against other data sources, such ePACT2. In addition, adding tests is a worthwhile investment. With tests you can be more confident that any future change that introduces an error will be caught quickly.\nIf the data is something that you would expect personal details to appear in, you can also confirm you see what you would expect for yourself."
  },
  {
    "objectID": "guidance/03-analysis/analysis-steps/03-output-artifacts.html",
    "href": "guidance/03-analysis/analysis-steps/03-output-artifacts.html",
    "title": "Output artifacts",
    "section": "",
    "text": "The expected outputs of an initiative will have been agreed in the problem definition. These will range from written reports, summary slide decks, data sets and applications such as dashboards or interactive reports. Getting frequent feedback is important, as it helps to prevent drift from the problem definition and allows more people to spot problems as early as possible."
  },
  {
    "objectID": "guidance/03-analysis/analysis-steps/03-output-artifacts.html#supporting-text",
    "href": "guidance/03-analysis/analysis-steps/03-output-artifacts.html#supporting-text",
    "title": "Output artifacts",
    "section": "Supporting text",
    "text": "Supporting text\nA written report could be be done in Word or markdown (plain or Rmarkdown, depending on need to run and include code and code output). Using markdown the output could be rendered as either Word or PDF. Alternatively, the output may be in the form of an app, such as a Quarto site, Rmarkdown HTML output or Shiny dashboard. Some style guidelines applicable to all these outputs include\n\nKey findings at the front as people won’t read the whole thing\nInclude caveats and assumptions, and for particularly important ones mention them up front\nNumbers under 10 are in words, over 10 in digits (but use common sense!)\nUse bold tag lines for emphasis of important points\nSometimes difficult to do, but try to have a story\nIf appropriate, include recommendations and ideas for further exploration\nSentences shorter than 20 words\nWrite it so that a 12 year old would be able to understand it\n\nFor a sense of cohesiveness, have one person be the editor. This does not mean they must write everything themselves from scratch. Often when working as a team individuals will be responsible for different parts of the whole piece. They should write up their own parts, which can then be re-written by the person taking the editor role. The whole team can then review and agree on a final version that speaks with one clear voice.\nA very useful tool for written output is Hemingway Editor. If you can get your text to mostly have no issues in this, you know it is readable.\nIt is advised to leave writing any text supporting visuals such as charts or tables until the team is almost certain they are finalised."
  },
  {
    "objectID": "guidance/03-analysis/analysis-steps/03-output-artifacts.html#slide-decks",
    "href": "guidance/03-analysis/analysis-steps/03-output-artifacts.html#slide-decks",
    "title": "Output artifacts",
    "section": "Slide decks",
    "text": "Slide decks\nAll initiatives should have a slide deck as an agreed output. At a minimum this should be detailed enough to state the problems or questions the initiative looked at, the approach used to address or answer these and a summary of the most important findings. Aim to have most of the content be visual, with limited amounts of explanatory text.\nWhen creating a slide deck it is important to consider the audience. Overcrowding the slides with technical detail would not be suitable for a non-technical audience.\nOften times it is difficult to condense a whole initiative into a single short presentation. If the circumstances allow it, e.g. a conference talk, focus on a single clear message. Split the talk into three key sections and messages that you want somebody to remember about your project. A good presentation style to get a clear message across is the newsreader style."
  },
  {
    "objectID": "guidance/03-analysis/analysis-steps/03-output-artifacts.html#applications",
    "href": "guidance/03-analysis/analysis-steps/03-output-artifacts.html#applications",
    "title": "Output artifacts",
    "section": "Applications",
    "text": "Applications\nShiny powered applications should use the template provided by the nhsbsaShinyR repository. This repo is continually being refined, so will include code already tested (including accessibility testing). Using the same starting point also means our Shiny outputs will have a single easily recognisable look and feel.\nThe other platform we use to deliver output as an application is Power BI. This platform is much newer, so best working practices are still being discovered. Similarly to the nhsbsaShinyR template for Shiny, there is a template for use in Power BI.\nThere are communities of practice within the organisation for Data Visualisation and for Power BI. These are good places to start with any questions you have while creating applications, or other means of data visualisation.\n\nRAP and development considerations\nDon’t repeat yourself is a well known adage in software development. While we are not writing software, it still applies and is a core principle of RAP. Pull out repeated code into functions; this allows for good tests to be written, another core principle of RAP. Variables should not be spread throughout a script, but instead gathered together into a config file or single block at the top of a script.\nThe nhsbsaShinyR template for Shiny is a good starting point for creating the app with RAP in mind. An excellent book on the subject of Shiny apps is Engineering Production-Grade Shiny Apps. Whilst not written specifically with RAP in mind, many of the principles and techniques are relevant. Following the advice and techniques in this book should produce a very professional and well-planned app. The authors are the team behind the golem framework, ThinkR, used in the nhsbsaShinyR template.\nFor a more in-depth look at what can be done in Shiny, there is Outstanding User Interfaces with Shiny. This is particularly helpful when trying to address an accessibility failing of some content.\n\n\nIssue tracking\nIt is recommended to use GitHub issues for tracking essential and potential changes or fixes. Issues allow easy creation of branches and pull requests, keeping the non-coding tasks all in one place.\n\n\nTesting and code reviews\nWhen some set of changes or fixes is ready, make sure to run the whole app and go through any content related to the change. Add tests as you add functionality, you will thank yourself later! When code reviewing, always download and run the app. This catches issues related to something being present in someones environment, which is no longer there when someone else runs it."
  },
  {
    "objectID": "guidance/03-analysis/analysis-steps/03-output-artifacts.html#data-set-outputs",
    "href": "guidance/03-analysis/analysis-steps/03-output-artifacts.html#data-set-outputs",
    "title": "Output artifacts",
    "section": "Data set outputs",
    "text": "Data set outputs\nSometimes the customer requires a regularly updated data set to be provided as an output. Done with little thought, this can result in unnecessary overhead that continues indefinitely. It is recommended to productionise the process as much as it can be, in line with the principles of RAP."
  },
  {
    "objectID": "guidance/03-analysis/analysis-steps/04-team-review.html",
    "href": "guidance/03-analysis/analysis-steps/04-team-review.html",
    "title": "Team review",
    "section": "",
    "text": "Frequent reviews of the work give a chance to pick up on any issues before asking third parties to look at it. An important component of this is code review. With the mindset that the code may become public, put yourself in the shoes of someone coming across the code and wanting to both understand it and then reuse it. This means keeping in mind the principles of Reproducible Analytical Pipelines (RAP), which benefits others coming across the code in future, but also the team themselves while continuing to work on it.\nThe basic idea of the analysis loop is a tight cycle of refine and get feedback. Before asking for feedback, the team needs to ensure that they have checked everything is as they expect it to be. This includes code and any supporting text or other artifacts such as data sets, slide decks etc. By doing the analysis in a reproducible way, using RAP principles, this continuous improvement can be made to run smoothly."
  },
  {
    "objectID": "guidance/03-analysis/analysis-steps/04-team-review.html#is-the-code-in-a-good-state",
    "href": "guidance/03-analysis/analysis-steps/04-team-review.html#is-the-code-in-a-good-state",
    "title": "Team review",
    "section": "Is the code in a good state?",
    "text": "Is the code in a good state?\n\nTesting\nConsider using a test driven development (TDD) approach to writing functions. This has many benefits but comes with the drawback of investing time up-front, with no guarantee that it will be useful. TDD means knowing immediately when something is wrong and where the bug occurs. In the long run, the time saved will outweigh the up-front investment.\nEven if not using TDD, you should be testing code as you develop it. Since you are testing, spend a bit longer on making these tests reproducible automatically.\nFor R, it is recommended to use the testthat package.\nFor python, use of ?? is recommended.\n\n\nComments\nIn general, the purpose of code should be apparent to someone who is familiar with the language. If it is not, first try to find a more readable and clear way to write it. If not, code comments should be present to explain it. Also, you can split code into sections. For example in Rstudio by using Ctrl+Shift+R.\n\n\nDocumentation\nPeople other than the authors may look at and use our code. This could be other members of the team, colleagues in the NHSBSA or the public. Our code should be well documented. This is both professional and increases ease of reuse and maintenance.\nFor R, it is recommended to use the roxygen2 package. See chapter 16 of R Packages (2e) for details by the author of roxygen2.\nFor python, use of ?? is recommended."
  },
  {
    "objectID": "guidance/03-analysis/analysis-steps/04-team-review.html#textual-content",
    "href": "guidance/03-analysis/analysis-steps/04-team-review.html#textual-content",
    "title": "Team review",
    "section": "Textual content",
    "text": "Textual content\nA common component of the outputs we create is text. Be aware that the refine-feedback cycle can be challenging if you have such text in multiple places. For example, in a Word document used for review, but also in a shiny dashboard. Try to keep all text in a single place until as late as possible in the initiative life cycle.\nSpecifically for R, there is text review functionality built-in to the nhsbsaShinyR template. If you have this installed, see the vignette on this by running vignette(\"Text review\", \"nhsbsaShinyR\"). This allows for using a Word document initially, the contents of which can be automatically used to create markdown content for the app. It works the other way also, to provide a Word document constructed from existing markdown files. This makes syncing the text when it has changed due to review feedback back into the app a mostly automatic process."
  },
  {
    "objectID": "guidance/03-analysis/analysis-steps/04-team-review.html#accessibility-testing",
    "href": "guidance/03-analysis/analysis-steps/04-team-review.html#accessibility-testing",
    "title": "Team review",
    "section": "Accessibility testing",
    "text": "Accessibility testing\nIf your output is to be available online, it must be tested for accessibility. Full details are on the DALL wiki.\nBear in mind that testing accessibility too early can result in wasted time, as aspects of the app can and do change. So in general, leave such testing until fairly confident that only minor adjustments and text agreement remain to be done."
  },
  {
    "objectID": "guidance/03-analysis/analysis-steps/05-critical-friend-review.html",
    "href": "guidance/03-analysis/analysis-steps/05-critical-friend-review.html",
    "title": "Critical friend review",
    "section": "",
    "text": "The critical friend is an integral part of the team. They serve as someone to bounce ideas off, offer advice and act as another pair of eyes and ears that may pick up up on something unnoticed by the rest of the team. So keep them in the loop as much as possible by asking them to review things before taking anything to the customer of the initiative.\nThe critical friend should review the process, code and outputs of the team. This should be done before asking the initiative customer to review the outputs. The ask is always “be as thorough as you can”, but other commitments can make a thorough review challenging. So, where possible, give the critical friend advance notice that you expect to be ready for their review soon.\nAt a minimum, the critical friend should be\n\nPulling code to their local environment and running it\nReading supporting text and commenting on it where they are unsure of something, believe a mistake is there or have a suggestion\nChecking the outputs against the problem definition and calling out any discrepancies"
  },
  {
    "objectID": "guidance/03-analysis/analysis-steps/05-critical-friend-review.html#flexible-reviewing",
    "href": "guidance/03-analysis/analysis-steps/05-critical-friend-review.html#flexible-reviewing",
    "title": "Critical friend review",
    "section": "Flexible reviewing",
    "text": "Flexible reviewing\nSince the critical friend will be working on other initiatives, it can be challenging to coordinate reviews. So we should do things in a way that is as flexible as possible, while also keeping on top of any timescales promised. One way to promote this is to make it possible for the critical friend to review work in progress.\nGitHub allows for the creation of draft pull requests (PRs). By using these at the beginning of a review cycle, and ensuring to always check our work into GitHub at least daily, the work can be reviewed when it is convenient. Some may prefer to dip in and out to fit around other commitments, while others would prefer to review things only when fully developed."
  },
  {
    "objectID": "guidance/03-analysis/analysis-steps/06-customer-review.html",
    "href": "guidance/03-analysis/analysis-steps/06-customer-review.html",
    "title": "Customer review",
    "section": "",
    "text": "Before asking the customer to review and feedback on the work, ensure that whichever output they are looking at is either complete according to the problem definition, or for work in progress that you have made it clear what points on the problem definition are (or are not) being covered so far. Use the feedback to help steer things back on course if necessary. Something that can occur is the customer now asking for something different, or entirely new. This should be handled with care, as over-committing can backfire. In such cases it may be better to propose a future phase of work for any additional outputs requested."
  },
  {
    "objectID": "guidance/03-analysis/index.html",
    "href": "guidance/03-analysis/index.html",
    "title": "Analysis",
    "section": "",
    "text": "In this stage we prepare data, analyse it and review until outputs as specified in the problem definition are finalised. It is sometimes necessary to go back and refine the problem definition as a result of discoveries or issues found during these tasks."
  },
  {
    "objectID": "guidance/04-wrap-up/index.html",
    "href": "guidance/04-wrap-up/index.html",
    "title": "Wrap-up",
    "section": "",
    "text": "Add text here\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData preparation\n\n\nPrepare data and create base table(s)\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "guidance/04-wrap-up/wrap-up-steps/01-step.html",
    "href": "guidance/04-wrap-up/wrap-up-steps/01-step.html",
    "title": "Data preparation",
    "section": "",
    "text": "The goal of this step is to end up with one or more base tables on which analysis is then conducted. The base tables should include only data relevant, or possibly relevant, to the initiative. Some tables can have hundreds of columns, so reducing the footprint in your base tables is a recommended. Data preparation may be iterative, as the analysis becomes refined with frequent feedback from stakeholders.\nThe tables relevant to the initiative will have been identified in the process of writing the problem definition. Before using them it is important to get an idea of the data quality. This consists of summary statistics including at least\n\nData type\nMost common values for categorical fields, or minima/maxima and median for numeric\nCompleteness\nCross-correlation\n\nThere is the possibility of creating a SQL macro or stored procedure to generate the summary statistics in Oracle itself. This will streamline the process and is reproducible and promotes a consistent treatment of data quality.\n\n\nThis is the data type as stored. This may or may not be the same as what you expect. For example, it is common to use a numeric key in a fact table that represents a categorical value found in a dimension table. See the Wiki for more on fact and dimension tables (TODO).\n\n\n\nFor categorical fields, it is useful to have an idea of the distribution of values. For example, if considering use of machine learning techniques that are sensitive to imbalanced data. Even when considering techniques that handle imbalanced data well keeping the distribution in mind is useful when interpreting and reporting on the the results.\n\n\n\nFor numeric fields, this is the analogue to common values for categorical data. The values will give you an idea of how the data is distributed. You could go further and plot a histogram for better visualisation of this. NOTE: Consider adding to Rmarkdown example\n\n\n\nMany techniques are sensitive to missing data. Extreme cases of this can even prevent any useful analysis, so needs to be mitigated in some way. How it is mitigated will depend on the techniques employed. One thing to look out for is cases where a field has values only after, or before, a certain time. When this occurs it may limit some specific analysis to the time period for which values exist.\n\n\n\nSome techniques are resistant to correlation between explanatory variables, while others require all but one such correlated variable to be removed. In either case it is useful to understand what, if any, correlation between variables exists. The correlations can be computed with R, python or directly in Oracle. See these guides for more info\nR Python Oracle\n\n\n\nThe template initiative Data folder contains a Rmarkdown, data_summary.Rmd, which can be used as an example or starting point for data quality checking. When using it make sure to add notes of the findings for future reference.\nAn alternative would be to do it directly in the database, such as suggested on this github issue.\nIn addition to any Rmarkdown used, you may find it helpful to keep a record of specific counts found during initial EDA and during any further EDA performed while preparing the base tables. An example Excel file is in the template initiative Data folder, EDA figures.xlsx."
  },
  {
    "objectID": "guidance/04-wrap-up/wrap-up-steps/01-step.html#data-cleansing",
    "href": "guidance/04-wrap-up/wrap-up-steps/01-step.html#data-cleansing",
    "title": "Data preparation",
    "section": "Data cleansing",
    "text": "Data cleansing\n\nMissingness\nThe data may have some degree of missingness. How it is handled will depend on the both the data and the techniques to be employed in the analysis. There are many techniques to handle this. Sometimes the best method is obvious, sometimes you will have to choose between several potentially viable methods.\n\n\nData types\nFrom the data summary generated you may find some fields that are better expressed in a different data type.\n\n\nKeep or discard fields\nIn the first iteration it is best to keep any and all fields that may be useful in the analysis. As the analysis is refined through exploration and feedback, you can discard fields no longer deemed relevant. By the end, only fields which are used in the analysis should remain in the base tables. An exception to this is when a future phase of work is planned. In such cases it may make sense to keep some fields not immediately useful available."
  },
  {
    "objectID": "guidance/04-wrap-up/wrap-up-steps/01-step.html#rap-considerations",
    "href": "guidance/04-wrap-up/wrap-up-steps/01-step.html#rap-considerations",
    "title": "Data preparation",
    "section": "RAP considerations",
    "text": "RAP considerations\nDon’t repeat yourself is a well known adage in software development. While we are not writing software, it still applies and is a core principle of RAP. Pull out repeated code into functions; this allows for good tests to be written, another core principle of RAP. Variables should not be spread throughout a script, but instead gathered together into a config file or single block at the top of a script."
  },
  {
    "objectID": "guidance/04-wrap-up/wrap-up-steps/01-step.html#note-keeping",
    "href": "guidance/04-wrap-up/wrap-up-steps/01-step.html#note-keeping",
    "title": "Data preparation",
    "section": "Note keeping",
    "text": "Note keeping\nThroughout data preparation, ensure to make notes (saved in the initiative folder) of any potential or realised issues, caveats to be included in final outputs and decisions made with their reasoning. This will help in writing the supporting text in the main outputs."
  },
  {
    "objectID": "guidance/04-wrap-up/wrap-up-steps/01-step.html#validation",
    "href": "guidance/04-wrap-up/wrap-up-steps/01-step.html#validation",
    "title": "Data preparation",
    "section": "Validation",
    "text": "Validation\nSome thought should be given to how to validate the base tables are a true reflection of what is intended. Some data may be checked against other data sources, such ePACT2. In addition, adding tests is a worthwhile investment. With tests you can be more confident that any future change that introduces an error will be caught quickly.\nIf the data is something that you would expect personal details to appear in, you can also confirm you see what you would expect for yourself."
  },
  {
    "objectID": "guidance/04-wrap-up/wrap-up-steps/01-step.html#miro-points-for-reference-to-be-removed",
    "href": "guidance/04-wrap-up/wrap-up-steps/01-step.html#miro-points-for-reference-to-be-removed",
    "title": "Data preparation",
    "section": "Miro points (for reference, to be removed)",
    "text": "Miro points (for reference, to be removed)\nKeep small spreadsheet to keep track of all EDA results\nAlso links with business understanding\nInvest the time into creating accurate base tables\nEither R or SQL for her analysis, just whatever she feels like\nR markdown is quite good to lift into presentations\nUses data miner to extract insights and aggregate SQL tables with the info\nDALP data is mostly snapshot data, some get automatically refreshed, otherwise we ask for it, get it from DWCP or external system\nCan do SQL script and put all results into Excel\nSQL to do base tables, get all the columns you need and cleaned. Then analysis in R\nCould pull base table into R via DBI or DBPLYR\nKeep detailed notes as you progress - Excel, Rmd, PowerPoint. This will speed up creating your outputs"
  },
  {
    "objectID": "index.html#sections",
    "href": "index.html#sections",
    "title": "NHSBSA DALL Playbook",
    "section": "Sections",
    "text": "Sections"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This website has been developed by the NHSBSA Data and Analytics Learning Laboratory (DALL) team to provide guidance and processes for producing new initiatives."
  },
  {
    "objectID": "guidance/01-pre-pd/index.html#sections",
    "href": "guidance/01-pre-pd/index.html#sections",
    "title": "Before the problem definition",
    "section": "Sections",
    "text": "Sections"
  },
  {
    "objectID": "guidance/01-pre-pd/pre-pd-steps/01-problem-identified.html",
    "href": "guidance/01-pre-pd/pre-pd-steps/01-problem-identified.html",
    "title": "Hypothesis identified",
    "section": "",
    "text": "The Data Science team deliver actionable insights from data through innovation, experimentation and collaboration. The insights drive policy, decision making and efficiencies across the NHSBSA and wider health and social care system. Projects typically align to the following areas:"
  },
  {
    "objectID": "guidance/01-pre-pd/pre-pd-steps/02-assign-critical-friend.html",
    "href": "guidance/01-pre-pd/pre-pd-steps/02-assign-critical-friend.html",
    "title": "Assign a critical friend",
    "section": "",
    "text": "Typical Data Science initiatives involve 1-2 people working on the project.\nFor each project a “critical friend” is assigned.\nA critical friend reviews the code and/or business understanding.\nThey can also attend meetings, check documents and provide extra support and guidance throughout the initiative."
  },
  {
    "objectID": "guidance/01-pre-pd/pre-pd-steps/02-assign-critical-friend.html#what-is-a-critical-friend",
    "href": "guidance/01-pre-pd/pre-pd-steps/02-assign-critical-friend.html#what-is-a-critical-friend",
    "title": "Assign a critical friend",
    "section": "",
    "text": "Typical Data Science initiatives involve 1-2 people working on the project.\nFor each project a “critical friend” is assigned.\nA critical friend reviews the code and/or business understanding.\nThey can also attend meetings, check documents and provide extra support and guidance throughout the initiative."
  },
  {
    "objectID": "guidance/01-pre-pd/pre-pd-steps/02-assign-critical-friend.html#tips-for-working-with-your-critical-friend",
    "href": "guidance/01-pre-pd/pre-pd-steps/02-assign-critical-friend.html#tips-for-working-with-your-critical-friend",
    "title": "Assign a Critical Friend",
    "section": "Tips for Working With Your Critical Friend",
    "text": "Tips for Working With Your Critical Friend\nIt is useful to assign a critical friend as early as possible. Ideally, they should have prior experience with similar data or techniques. This will help them offer more insight and review the project. If you are an initiative expert, you can choose a new team member as the critical friend so they can gain experience.\nMake sure to include them in the project through regular catch ups. This way, they can review your work gradually, instead of all at once. If you can, make sure to give your critical friend as much as notice as possible of work that you will be sending over, to ensure they can make the time to look at it."
  },
  {
    "objectID": "guidance/01-pre-pd/pre-pd-steps/03-get-access-to-data.html",
    "href": "guidance/01-pre-pd/pre-pd-steps/03-get-access-to-data.html",
    "title": "Get access to the data and undertake initial understanding",
    "section": "",
    "text": "Temporary access must be requested via email or a service request and is subject to approval from the Data Science Lead, Database administrators and/or Data Asset Owner.\nShould sensitive data be needed, there may also be a requirement to engage with the Caldicott Guardian and/or Information Governance.\nAll data must be accessed and processed through the Data Science secure data environment.\nAlso request any relevant metadata, data dictionaries and data quality profiles."
  },
  {
    "objectID": "guidance/01-pre-pd/pre-pd-steps/03-get-access-to-data.html#usage",
    "href": "guidance/01-pre-pd/pre-pd-steps/03-get-access-to-data.html#usage",
    "title": "Get access to the data and undertake initial understanding",
    "section": "Usage",
    "text": "Usage\nFurther advice on usage of specific databases can be found within the internal Data Science OneNote Wiki."
  },
  {
    "objectID": "guidance/01-pre-pd/pre-pd-steps/04-initial-research.html",
    "href": "guidance/01-pre-pd/pre-pd-steps/04-initial-research.html",
    "title": "Desk Research and Business Understanding",
    "section": "",
    "text": "Desk research and business understanding will help you understand what is possible for the initiative and ensure the work is not duplicated.\nTo assist your work you could use:\n\nJIRA as a reference to previous initiatives\nreports and other outputs from previous initiatives in the project folders\nother team members, business SMEs and your critical friend to help develop business knowledge\ninternal and external online resources which may describe the service, policy or other aspects relevant to understanding context\nexternally or internal published statistics, data or dashboards relevant to the subject. They may also later be a source for validation and ensuring coherence.\nresources to support with methodology such as Github repositories, Stack Overflow, Medium and research papers\nDataCamp to complete relevant courses\n\nThis is a guide of what is possible and is dependent on the current project’s requirements."
  },
  {
    "objectID": "guidance/01-pre-pd/pre-pd-steps/05-initial-meeting-with-customer.html",
    "href": "guidance/01-pre-pd/pre-pd-steps/05-initial-meeting-with-customer.html",
    "title": "Meeting with the Customer",
    "section": "",
    "text": "It is important to meet with the customer to discuss requirements. This may happen before and/or after you develop initial business and data understanding. During this meeting, you can identify:"
  },
  {
    "objectID": "guidance/01-pre-pd/pre-pd-steps/06-create-initiative-resources.html",
    "href": "guidance/01-pre-pd/pre-pd-steps/06-create-initiative-resources.html",
    "title": "Create Initiative Resources",
    "section": "",
    "text": "Before beginning to work on your problem definition, you will need to setup the initiative resources. Currently, these include:"
  },
  {
    "objectID": "guidance/01-pre-pd/pre-pd-steps/06-create-initiative-resources.html#tips-for-setting-up-the-resources",
    "href": "guidance/01-pre-pd/pre-pd-steps/06-create-initiative-resources.html#tips-for-setting-up-the-resources",
    "title": "Create Initiative Resources",
    "section": "Tips for Setting Up the Resources",
    "text": "Tips for Setting Up the Resources\nBefore creating these, you will need to identify the DALL number of the initiative. To do this, and for guidance on how to setup the above, please see the DALL OneNote Wiki page here.\nWhen setting up a Github repository, remember to set it to private and invite the DALL team to the project so everyone can make changes. You should write your code with the intention of the repo going public in the future. Therefore, you must be careful not to include PII information within your code, such as NHS numbers.\nAfter creating all resources, remember to provide regular updates to the ticket and repository. This allows the DALL team manager to track progress."
  },
  {
    "objectID": "guidance/02-pd/index.html#sections",
    "href": "guidance/02-pd/index.html#sections",
    "title": "Problem definition",
    "section": "Sections",
    "text": "Sections"
  },
  {
    "objectID": "guidance/02-pd/pd-steps/01-data-business-understanding.html",
    "href": "guidance/02-pd/pd-steps/01-data-business-understanding.html",
    "title": "Writing the Problem Definition",
    "section": "",
    "text": "An agreement between the Data Scientists and the customer(s).\nIt defines a set of goals that everyone agrees on before work begins.\nIt ensures there is a common understanding.\nIt should be stored within the SharePoint folder for reference.\n\nAs part of writing the problem definition, you will need to include:\n\nsome background information about the area\nwhat it is that needs to be solved\nhow we intend to solve the problem\nany limitations and caveats, if they exist\nwhat tools and data we will use\nany information governance and ethical considerations, if they are applicable"
  },
  {
    "objectID": "guidance/02-pd/pd-steps/01-data-business-understanding.html#tips-for-gaining-understanding",
    "href": "guidance/02-pd/pd-steps/01-data-business-understanding.html#tips-for-gaining-understanding",
    "title": "Data and business understanding",
    "section": "Tips for gaining understanding",
    "text": "Tips for gaining understanding\n\nIt is a good idea to keep a list of all questions you have with the data.\nFor further insight, check JIRA and ask around to see if anyone has explored this data before.\nThe Critical friend and team manager can also provide assistance.\n\nFinally, it is possible that other analytical teams within the BSA have worked with the data before and may be able to offer insight.\n\nThe Management Information (MI) team produce a range of dashboards in a variety of areas.\nThe External Reporting team manage the ePACT2 (prescriptions), eDEN (dental) and eOPS (ophthalmic) systems."
  },
  {
    "objectID": "guidance/02-pd/pd-steps/02-customer-feedback.html",
    "href": "guidance/02-pd/pd-steps/02-customer-feedback.html",
    "title": "Customer feedback",
    "section": "",
    "text": "After finishing your draft of the problem definition (PD), meet with the customer(s) to go through it together and:\nBe sure to:\nAs an outcome of the meeting, there may be additional things to explore or add to the PD before the customer can sign off."
  },
  {
    "objectID": "guidance/02-pd/pd-steps/03-refine-pd.html",
    "href": "guidance/02-pd/pd-steps/03-refine-pd.html",
    "title": "Refine the problem definition",
    "section": "",
    "text": "It is important to repeat the problem definition loop again until the customer is happy to proceed.\nIn rare cases, this meeting may highlight that the initiative may not be possible in the way that the customer initially intended.\nUnless any suitable alternatives are identified, we may need to give up on the project.\nIn this case, you must discuss your concerns with the Data Science Lead before communicating this with the customer."
  },
  {
    "objectID": "guidance/02-pd/pd-steps/03-refine-pd.html#what-is-a-problem-definition",
    "href": "guidance/02-pd/pd-steps/03-refine-pd.html#what-is-a-problem-definition",
    "title": "Refine the problem definition",
    "section": "",
    "text": "An agreement between the assignees and the customer(s).\nIt defines a set of goals that everyone agrees on before work begins.\nIt protects both parties further down the line.\nIt should be stored within the SharePoint folder for reference.\n\nAs part of writing the problem definition, you will need to include:\n\nsome background information about the area\nwhat it is that needs to be solved\nhow we intend to solve the problem\nany limitations and caveats, if they exist\nwhat tools and data we will use\nany data governance considerations, if they are applicable"
  },
  {
    "objectID": "guidance/02-pd/pd-steps/03-refine-pd.html#tips-for-writing-a-pd",
    "href": "guidance/02-pd/pd-steps/03-refine-pd.html#tips-for-writing-a-pd",
    "title": "Refine the Problem Definition",
    "section": "Tips For Writing a PD",
    "text": "Tips For Writing a PD\nFor sensitive data containing personally identifiable information (PII), you will need to get approval from the Information Governance (IG) team before you start the work. During this process, you will need to tell them what sensitive data you are using and how you intend to use it. Your critical friend and DALL team manager can assist with this process. For more information, please see the following page within the wiki [INSERT LINK].\nFor tips on writing the content of the PD, please see the DALL OneNote Wiki [INSERT LINK]. To make sure customers understand our plan, it’s best to include a lot of details in the PD. If any further caveats are discovered during the work, it is important to add these to the PD and make the customer aware. If you need to do more work after starting the initiative, update the plan and tell the customer about the extra time needed. If you are unsure about any terms agreed within the PD, you can reach out to your critical friend or the DALL team manager for advice.\nThroughout the process, your critical friend will review the PD to check if you understood the customer’s request correctly, and if your plans to solve the problems are appropriate. Also, make sure to share the PD with the customer(s) to get their feedback, understand their business needs, and make sure you have addressed all their requests."
  },
  {
    "objectID": "guidance/02-pd/pd-steps/04-estimate-timescales.html",
    "href": "guidance/02-pd/pd-steps/04-estimate-timescales.html",
    "title": "Estimate the timescales",
    "section": "",
    "text": "During the planning stages of the initiative, it is a good idea to plan the timescales of the project.\nTo plan effectively:\n\ncreate a Gantt chart\nlist the items and their completion times\nestimate when each element will be finished\n\nPlease see the Gantt chart template within the templates folder on SharePoint.\nWhen planning any project, it is easy to underestimate the time required to complete it.\nYou should always account for:\n\ncommitments in other projects\nupcoming planned leave\ntime spent in meetings per week\nthe hours you work per week\ntime spent learning about the project\ntime required to get access to tools and data\nany other commitments\n\nIf you add more tasks to the project, make sure to update the Gantt chart and inform the customer of the new deadlines. Your planned deadlines are usually just estimates and can be extended if underestimated."
  },
  {
    "objectID": "guidance/03-analysis/analysis-steps/01-data-prep.html#check-the-data-quality",
    "href": "guidance/03-analysis/analysis-steps/01-data-prep.html#check-the-data-quality",
    "title": "Data Preparation",
    "section": "Check the Data Quality",
    "text": "Check the Data Quality\nThe tables relevant to the initiative will have been identified in the process of writing the problem definition. Before using them, it is important to get an idea of the data quality. To do this, it is advisable to keep a log, saved in the initiative folder, for each table that you have access to. This could be done in Excel, R Markdown, PowerPoint, or any other software you see fit.\nWithin the log, you can track the number of populated values, missing values, and data errors per column. Also, it is useful to extract some summary statistics for each table such as the min, max, and median of each numerical column, and distributions of categorical columns. It is also beneficial to have a notes tab to remind yourself what each column does and any things to watch out for. Finally, you can use the additional tabs to paste in data results from SQL, R or Python to highlight issues or findings within the data. Keep a list of these so you can show them to the customer and your critical friend. This will make them aware and help find solutions together. Please see the DALL OneNote Wiki for more information on checking data quality [INSERT LINK].\nThroughout this process, it is crucial to test and validate your results where possible. A good tip is to search for yourself in the datasets, if you know your NHS or NI number, to check if your data is represented correctly. With the data and business knowledge from earlier, you can search for specific scenarios to test. With specific datasets, you can check previous DALL initiatives or MI dashboards to compare you figures. Finally, for prescriptions data specifically, you can compare your results to ePACT2. For more information on testing, please see this page within the DALL OneNote Wiki [INSERT LINK]."
  },
  {
    "objectID": "guidance/03-analysis/analysis-steps/01-data-prep.html#create-the-base-tables",
    "href": "guidance/03-analysis/analysis-steps/01-data-prep.html#create-the-base-tables",
    "title": "Data Preparation",
    "section": "Create the Base Table(s)",
    "text": "Create the Base Table(s)\nOnce you have completed the data log, you will be in the position to transform your data into a base table or series of tables. Within this step, based on the results of your exploration, you will:\n\ndecide to keep or discard fields based on what is relevant to the initiative,\nfix or remove columns that contain data errors or missing entries,\ncreate new columns based on the existing ones,\nchange the data types of the columns to a more appropriate format and\nsave the output of your transformations as a table or a series of tables.\n\nFor the final step, this can be as simple as saving your results as CSV files or creating SQL tables, depending on the size of your data. As a final step, you must remove all sensitive info from the base table(s) where possible to avoid data leakage. Finally, you can use cloud software to schedule data preparation and rerun it automatically. For further guidance on these steps, including how to tackle missing and erroneous data, please see the DALL OneNote Wiki [INSERT LINK]."
  },
  {
    "objectID": "guidance/03-analysis/analysis-steps/02-analysis-modelling.html#conducting-the-work",
    "href": "guidance/03-analysis/analysis-steps/02-analysis-modelling.html#conducting-the-work",
    "title": "Analysis and modelling",
    "section": "Conducting the work",
    "text": "Conducting the work\nStarting from the base tables, we want to create the agreed outputs. Depending on the work, there are many tools we can use to do this.\n\nR is especially suited to use for data analysis and visualisation, via Rmarkdown and Shiny.\nPython has most widespread support for machine learning and is more widely integrated with cloud based platforms.\nPower BI is a possible alternative to R for creating dashboards, but due to licensing may not be suitable for certain end users.\nIf it is appropriate, such as needing to show simple summary statistics, Excel and/or SQL can be used.\n\nPlease see the DALL wiki for tips on how to use all of these tools."
  },
  {
    "objectID": "guidance/03-analysis/analysis-steps/02-analysis-modelling.html#check-the-quality",
    "href": "guidance/03-analysis/analysis-steps/02-analysis-modelling.html#check-the-quality",
    "title": "Analysis and modelling",
    "section": "Check the quality",
    "text": "Check the quality\nAs with data preparation, it’s important to test and validate your results.\n\nWhere possible, check results against other sources.\nOther sources could be data from previous initiatives, Management Information dashboards or ePACT2.\n\nAdditionally, you must be careful about visualising sensitive results. Please contact your critical friend or the DALL team manager if you are unsure."
  },
  {
    "objectID": "guidance/03-analysis/analysis-steps/03-reporting-and-visuals.html",
    "href": "guidance/03-analysis/analysis-steps/03-reporting-and-visuals.html",
    "title": "Reporting and visuals",
    "section": "",
    "text": "Within the DALL team, the work that we conduct aims to tell a ‘story’ to the customer, providing them with the insights that the raw data alone would not."
  },
  {
    "objectID": "guidance/03-analysis/analysis-steps/03-reporting-and-visuals.html#producing-the-output",
    "href": "guidance/03-analysis/analysis-steps/03-reporting-and-visuals.html#producing-the-output",
    "title": "Reporting and Visuals",
    "section": "Producing the Output",
    "text": "Producing the Output\nAs discussed in the previous section, it may be appropriate to simply display your statistics in Excel if it is a small ask. Typical DALL initiatives show outputs in PowerPoint slides or a Markdown or Shiny report. With PowerPoint slides, it is advisable to create two versions of your output. One for the customer to read and one for you to show. The second one should have animation, fewer words, and pictures. It should ideally take no more than 10 minutes, giving time for questions. Also, if the customer can access it, they may find a Power BI dashboard helpful. The dashboard lets them analyse the data and gain insights themselves. Though, it would still be advisable to generate a set of slides, to talk the customer through the story of the initiative. A good tip for these tools is to include your graphs first and have then reviewed, before adding the text to save unnecessary work."
  },
  {
    "objectID": "guidance/03-analysis/analysis-steps/03-reporting-and-visuals.html#output-format",
    "href": "guidance/03-analysis/analysis-steps/03-reporting-and-visuals.html#output-format",
    "title": "Reporting and Visuals",
    "section": "Output Format",
    "text": "Output Format\nFor any reporting method you choose, it is recommended to have the following structure:\n\nIntroduction,\nContents,\nKey findings (since some may not have time to read the whole content),\nAssumptions and limitations,\nYour findings,\nRecommendations and next steps and\nAppendices (if applicable).\n\nFor the content of your outputs, you should always aim to make them understandable to anyone and make them accessible to view. Please keep your sentences under 20 words. Use bold and italics when necessary and write numbers under 10 using words. Ideally, one person would write and present the document, to keep it unified. If different writers use different styles in the same document, it will become disjointed and hard to present.\nTo learn more about improving initiative outcomes and getting specific software advice, visit the DALL OneNote Wiki pages [INSERT LINK]."
  },
  {
    "objectID": "guidance/03-analysis/analysis-steps/04-review.html",
    "href": "guidance/03-analysis/analysis-steps/04-review.html",
    "title": "Review",
    "section": "",
    "text": "Reviewing work and outputs frequently should be done throughout the initiative life cycle. There are multiple layers of review:\nIn response to any one of these reviews, it may be deemed necessary to go back to data preparation, analysis or drafting of outputs."
  },
  {
    "objectID": "guidance/03-analysis/analysis-steps/04-review.html#self-review",
    "href": "guidance/03-analysis/analysis-steps/04-review.html#self-review",
    "title": "Review",
    "section": "Self review",
    "text": "Self review\nReview your own work as you go, but especially before requesting anyone else to review it. Before submitting for review you should:\n\ncheck for errors\nrun code with a ‘clean slate’ to ensure there are no dependencies in your local environment that will not be there later\nconsider writing repeatable tests rather than adhoc testing\ndocument your code\ncreate a pull request (PR) on Github once happy with the code\ncheck the code diff (difference) on Github and add comments to explain to a code reviewer any decisions you made that are not obvious\ncheck the code diff as it may highlight something you missed; in these cases make further changes and push them\nmake sure each PR contains only small changes\n\nOnce happy with your PR, request that someone code review it.\n\n\n\n\n\n\nGive colleagues sufficient time\n\n\n\n\n\nRemember that your colleagues have other work; give them enough time to review your work properly."
  },
  {
    "objectID": "guidance/03-analysis/analysis-steps/04-review.html#team-review",
    "href": "guidance/03-analysis/analysis-steps/04-review.html#team-review",
    "title": "Review",
    "section": "Team review",
    "text": "Team review\nOne or more other members of the initiative team should review all code and outputs before it is merged into the main branch.\nThe reviewers should:\n\npull the code into their local environment, and run it from a ‘blank slate’ state\nadd comments and suggestions to the PR\ncheck any outputs match what was agreed in the problem definition\nask the code author to clarify if something does not make sense\norganise a call with the code author and work through the PR together if several questions arise"
  },
  {
    "objectID": "guidance/03-analysis/analysis-steps/04-review.html#critical-friend-review",
    "href": "guidance/03-analysis/analysis-steps/04-review.html#critical-friend-review",
    "title": "Review",
    "section": "Critical friend review",
    "text": "Critical friend review\nThe initiative critical friend (CF) should:\n\nbe asked for feedback on work in progress at frequent intervals, and always before asking a customer to look at it\nput themselves in the shoes of the customer when reviewing outputs\nput themselves in the shoes of someone tasked with using the code base when reviewing code\narrange a call with the initiative team to discuss if the feedback is complex or if there is lots of it"
  },
  {
    "objectID": "guidance/03-analysis/analysis-steps/04-review.html#customer-review",
    "href": "guidance/03-analysis/analysis-steps/04-review.html#customer-review",
    "title": "Review",
    "section": "Customer review",
    "text": "Customer review\nThe customer should be given frequent opportunities to see work in progress. To do this you should:\n\nsend them drafts of the outputs to review in their own time, rather than only sharing your screen in a call\nbe open to having calls once they have reviewed the work\ndiscuss any questions or suggestions they have\n\nWhen you feel the initiative is completed, you should have a call with the customer to present your findings and:\n\nensure the customer is happy they understand how to use any interactive elements, such as dashboards\ndiscuss any potential for future related work"
  },
  {
    "objectID": "guidance/03-analysis/index.html#sections",
    "href": "guidance/03-analysis/index.html#sections",
    "title": "Analysis",
    "section": "Sections",
    "text": "Sections"
  },
  {
    "objectID": "guidance/04-wrap-up/index.html#sections",
    "href": "guidance/04-wrap-up/index.html#sections",
    "title": "Initiative wrap up",
    "section": "Sections",
    "text": "Sections"
  },
  {
    "objectID": "guidance/04-wrap-up/wrap-up-steps/01-clean-up.html",
    "href": "guidance/04-wrap-up/wrap-up-steps/01-clean-up.html",
    "title": "Wrap up and clean your code",
    "section": "",
    "text": "Cleaning your project is an equally important step as the modelling and analysis.\nLeaving a messy project folder and code can lead to difficulties later down the line.\nIt is important that you dedicate enough time to completing this step.\nApproach the project clean up as a serious and worthwhile endeavour.\nIt can be useful for a future project."
  },
  {
    "objectID": "guidance/04-wrap-up/wrap-up-steps/01-clean-up.html#tips-for-clean-up-of-the-code",
    "href": "guidance/04-wrap-up/wrap-up-steps/01-clean-up.html#tips-for-clean-up-of-the-code",
    "title": "Wrap Up and Clean Your Code",
    "section": "Tips for Clean Up of the Code",
    "text": "Tips for Clean Up of the Code\nIf you are using Git, remember to work with the mindset that everything committed could be made public. Small and precise commits will make the project clean up easier to complete.\nEven if the code base will not be made public, you should take the time to clean it up and make sure all development branches are committed to main or deleted. Streamlining the codebase is crucial, as it makes it much easier to return to the work in the future. Additionally, it allows for other colleagues to follow the code more easily.\nBe sure to archive old scripts that are no longer required. For the remaining scripts, take the time to simplify and document the code. If you create a shiny dashboard as part of the project, then your code should already be structured well. If it isn’t, consider separating data, scripts, and output into separate sub-directories. Remember to check the DALL GitHub page to look up other projects for inspiration.\nIf you have written custom functions, it is a good idea to place these into a separate functions script which can then be called later in the project. Make sure to add comments to the functions. The comments should explain the expected input structure and the meaning of each parameter. It is also important to state what the output corresponds to. Including a simple example of how the function works can also be useful but is not mandatory.\nIf you intend to re-run the code in the future, consider creating a config or preamble script. The config script should contain all the key parameters and filters used in the project. For example, if you are filtering by a date range, you can define this in the config file and reference it throughout the project. This will speed up future work as only a single value needs editing before the code needs rerunning.\nCleaning code can be a subjective process. Remember to ask your critical friend for advice if you aren’t sure. When writing code, we should always try to make the code readable, which some R and Python packages can assist with. Additionally, the code should be optimised so that it can run as fast as possible. Therefore, we should always try and find a balance between performance and readability. Code should perform well and run quickly but also be understood by colleagues."
  },
  {
    "objectID": "guidance/04-wrap-up/wrap-up-steps/01-clean-up.html#tips-for-data-clean-up",
    "href": "guidance/04-wrap-up/wrap-up-steps/01-clean-up.html#tips-for-data-clean-up",
    "title": "Wrap Up and Clean Your Code",
    "section": "Tips for Data Clean Up",
    "text": "Tips for Data Clean Up\nIf you have key base tables created within DALP, make sure these are made available to the wider team. That way if you are away on annual leave or unavailable, other colleagues can access them. Often the key tables are moved to the DALL_REF schema at the end of the project make sure to remove any tables that you have created. Additionally, make sure temporary tables are deleted. Once a project concludes you shouldn’t have access to the data anymore, especially if it has personal information included.\nPlease see the DALL OneNote Wiki for more information on checking cleaning your code [INSERT LINK]."
  },
  {
    "objectID": "guidance/04-wrap-up/wrap-up-steps/02-write-documentation.html",
    "href": "guidance/04-wrap-up/wrap-up-steps/02-write-documentation.html",
    "title": "Write the documentation",
    "section": "",
    "text": "Without good documentation, it is difficult for future projects to leverage our work.\n\nMake sure that all of your custom functions are described in detail.\nConsider replicating a similar structure to an R or Python package with each parameter explained, and examples of how to run the function.\nIf using GitHub remember to update the README file.\n\nAt a minimum the GitHub README file should contain:\n\nthe project name\na quick overview of the project’s purpose\n\nAlso consider including:\n\na summary of the project structure\nsteps to reproduce your code\nkey technologies and methods used in the project\n\nRemember to document anything that you tried that did not work."
  },
  {
    "objectID": "guidance/04-wrap-up/wrap-up-steps/03-update-wiki.html",
    "href": "guidance/04-wrap-up/wrap-up-steps/03-update-wiki.html",
    "title": "Update the Wiki",
    "section": "",
    "text": "Throughout the initiative, you will have interrogated some data and potentially picked up some new skills in the process.\n\nIf there is anything you found particularly useful or that could be beneficial for others, please add it to the DALL OneNote Wiki.\nIf a question you have isn’t already on the Wiki you should find the answer and incorporate it.\n\nThe Wiki should be the first place you look when you have questions on a:\n\nparticular dataset\npiece of software\ntype of analysis"
  },
  {
    "objectID": "guidance/04-wrap-up/wrap-up-steps/04-public-repository.html",
    "href": "guidance/04-wrap-up/wrap-up-steps/04-public-repository.html",
    "title": "Make the repository public (if applicable)",
    "section": "",
    "text": "Where applicable, we aim to generate reproducible code that is made open source. This is in line with guidance outlined in the government service manual.\nBefore making a repository public, please consider the risks.\nYou must check the repository does not compromise data security before making it public.\n\n\n\n\n\n\nGit commit history\n\n\n\n\n\nYour git commit history will be made public.\n\n\n\nGit messages can be changed but it is advised that you follow good commit practice.\n\n\nEach commit should be limited to a single change.\n\n\nCommit history can be removed but it is not recommended as we strive to be transparent.\n\n\n\n\n\n\n\n\n\n\n\nNever expose Personally Identifiable Information (PII) on GitHub\n\n\n\n\n\nMake sure that you never push Personally Identifiable Information (PII) to the GitHub repository.\n\n\n\nUse Git Leaks to reduce the risk of pushing PII to Git.\n\n\nNever write PII directly in R scripts.\n\n\nAlways reference PII from the database."
  },
  {
    "objectID": "guidance/04-wrap-up/wrap-up-steps/05-future-work.html",
    "href": "guidance/04-wrap-up/wrap-up-steps/05-future-work.html",
    "title": "Document any future work",
    "section": "",
    "text": "After the project ended, you might have wanted to explore areas outside the project’s scope.\n\nAdd future project ideas to the DALL ideas register.\nShort pieces of work can be completed as a three week exploratory project."
  },
  {
    "objectID": "guidance/03-analysis/analysis-steps/01-data-prep.html#data-preparation",
    "href": "guidance/03-analysis/analysis-steps/01-data-prep.html#data-preparation",
    "title": "Data preparation",
    "section": "Data preparation",
    "text": "Data preparation\nData preparation can be done with any data manipulation tool such as SQL, R or Python.\n\nLarge datasets with millions of rows may require use of SQL initially.\nSQL can be used via libraries, such as {dplyr} and the database back-end {dbplyr} in R.\nOnce data has been aggregated, switch to your tool of choice for analysis and modelling.\n\nWhen preparing data there are some considerations that always apply.\n\nData quality\n\nCheck the Data Quality profiles, if they exist for the data.\nHow are values distributed? In particular are there any unbalanced data fields?\nCheck summary statistics such as mean, median, mode, minima and maxima.\nHow complete is the data? Any null or otherwise unknown values?\nAre any fields strongly correlated?\nDiscuss any data quality issues with the critical friend and/or customer.\n\nThere is a page on data quality in the internal DALL Wiki.\n\n\nData cleansing\n\nHandling missing values.\nUsing appropriate data types.\nFollowing the data minimisation principle Principle (c): Data minimisation | ICO.\n\nThere is a page on data cleansing in the internal DALL Wiki.\n\n\nRAP considerations (LINK?)\n\nIt is worth spending time on making the data preparation easy to repeat.\nRather than just doing adhoc testing, write formal tests that can be repeated.\nWorking in a RAP-ful way is especially important when the output of an initiative is to be refreshed periodically, such as monthly or annually.\nThere is a page on testing in the internal DALL Wiki.\n\n\n\nNote keeping\n\nSave the results of data quality checking.\nSave examples of queries that highlight any issues.\ndata_summary.Rmd and EDA figures.xlsx can be used, found in template initiative Data folder.\n\n\n\nValidation\n\nWhere possible, check that results using your base tables and other sources match.\nInternal sources include data from previous initiatives, Management Information dashboards or ePACT2.\nExternal sources include Official Statistics and third-party datasets."
  },
  {
    "objectID": "guidance/03-analysis/analysis-steps/01-data-prep.html#creating-base-tables",
    "href": "guidance/03-analysis/analysis-steps/01-data-prep.html#creating-base-tables",
    "title": "Data preparation",
    "section": "Creating base tables",
    "text": "Creating base tables\nAfter checking and preparing the data, you should have enough knowledge to create one or more base tables.\n\nDecide to keep or discard fields based on what is relevant to the initiative.\nFix or remove columns that contain data errors or missing entries.\nCreate new columns based on the existing ones.\nChange the data types of the columns to a more appropriate format.\nSave the output of your transformations.\nUse any format that makes sense considering the size and how it is used; formats include CSV, SQL, and Rda."
  },
  {
    "objectID": "guidance/03-analysis/analysis-steps/02-analysis-modelling.html#sensitive-data",
    "href": "guidance/03-analysis/analysis-steps/02-analysis-modelling.html#sensitive-data",
    "title": "Analysis and Modelling",
    "section": "Sensitive Data",
    "text": "Sensitive Data\n\nWherever data is saved, it must be in a secure location as per data security policy (LINK?).\nSensitive data should be removed or obfuscated unless it is absolutely necessary to retain it; bear in mind the data governance policies (LINK?).\nFor certain usage of sensitive data, the Caldicott Guardians (LINK?) should be consulted."
  },
  {
    "objectID": "guidance/03-analysis/analysis-steps/03-reporting-and-visuals.html#output-formats",
    "href": "guidance/03-analysis/analysis-steps/03-reporting-and-visuals.html#output-formats",
    "title": "Reporting and visuals",
    "section": "Output formats",
    "text": "Output formats\nThere are a range of expected outputs that may have been agreed on in the problem definition which can include:\n\nwritten reports\ndata sets\ndashboards\ninteractive reports\nslide decks (always required)"
  },
  {
    "objectID": "guidance/03-analysis/analysis-steps/03-reporting-and-visuals.html#output-structure",
    "href": "guidance/03-analysis/analysis-steps/03-reporting-and-visuals.html#output-structure",
    "title": "Reporting and visuals",
    "section": "Output structure",
    "text": "Output structure\nFor most outputs there is a recommended structure:\n\nIntroduction.\nContents.\nKey findings (since some may not have time to read the whole content).\nAssumptions and limitations.\nYour findings.\nRecommendations and next steps.\nAppendices (if applicable)."
  },
  {
    "objectID": "guidance/03-analysis/analysis-steps/03-reporting-and-visuals.html#supporting-text",
    "href": "guidance/03-analysis/analysis-steps/03-reporting-and-visuals.html#supporting-text",
    "title": "Reporting and visuals",
    "section": "Supporting text",
    "text": "Supporting text\nMost outputs will feature at least some text. General tips for writing includes:\n\nputting the key findings at the front as people won’t read the whole thing\nincluding caveats and assumptions, and for particularly important ones mention them up front\nwriting numbers under 10 as words, over 10 in digits (but use common sense!)\nusing bold tag lines for emphasis of important points\ndeveloping an interesting narrative (sometimes difficult to do)\nif appropriate, including recommendations and ideas for further exploration\nwriting sentences shorter than 20 words\nwriting so that a 12 year old would be able to understand it, assume readers are non-technical\nif multiple people have contributed text, assigning one person in the final stages to act as an editor and rewrite in a single style\nleaving the writing of any text that supports visuals such as charts or tables until the team is almost certain they are finalised\n\nThere are pages on initiative outputs with specific format advice in the internal DALL Wiki. A very useful tool for written output is Hemingway Editor. If you can get your text to mostly have no issues in this, you know it is readable!"
  },
  {
    "objectID": "guidance/03-analysis/analysis-steps/03-reporting-and-visuals.html#slide-decks",
    "href": "guidance/03-analysis/analysis-steps/03-reporting-and-visuals.html#slide-decks",
    "title": "Reporting and visuals",
    "section": "Slide decks",
    "text": "Slide decks\nAll initiatives should have at least two slide decks as an agreed output. One of these is for the customer, the other is designed to be quickly presented to interested parties.\n\nCustomer summary\nThis is to give the customer an easily shared and concise report on the outcomes of an initiative. The summary should:\n\nstate the problems or questions the initiative looked at, with examples\ngive the most important assumptions or caveats, and if appropriate issues encountered\nsummarise the most important findings\ngive brief recommendations and next steps\nprovide a mix of visuals and text, with more text preferred.\nuse technical language, if you think the customer is familiar with it\n\n\n\nShort presentation\nThis is intended to be used by the DALL team to present to others. It also serves as a quick reference of the outcome of an initiative, helpful to people looking back at it in future.\nA good presentation will:\n\nshow examples of the why the initiative was done\ngive assumptions and caveats only if absolutely necessary to prevent a major misunderstanding\nsummarise the most important findings\ngive brief recommendations and next steps\nlimit the amounts of explanatory text and make the most of visuals\nuse animations\ntake around 10 minutes to present (depending on situation)\nassume audience is non-technical\nfocus on up to three findings that you would like the audience to remember (applicable for large initiatives with lots of components and findings)\nuse a good presentation style such as the newsreader style\nhave an up to date final slide containing feedback and use cases\ngive people the opportunity to provide feedback- if you are not getting feedback ‘organically’, reach out to get enough for at least one slide!"
  },
  {
    "objectID": "guidance/03-analysis/analysis-steps/01-data-prep.html#sensitive-data",
    "href": "guidance/03-analysis/analysis-steps/01-data-prep.html#sensitive-data",
    "title": "Data preparation",
    "section": "Sensitive data",
    "text": "Sensitive data\n\nWherever base tables are saved, it must be in a secure location as per data security policy (LINK?).\nSensitive data should be removed or obfuscated unless it is absolutely necessary to retain it; bear in mind the data governance policies (LINK?).\nFor certain usage of sensitive data, the Caldicott Guardians (LINK?) should be consulted."
  },
  {
    "objectID": "guidance/03-analysis/analysis-steps/03-reporting-and-visuals.html#accessibility-concerns",
    "href": "guidance/03-analysis/analysis-steps/03-reporting-and-visuals.html#accessibility-concerns",
    "title": "Reporting and visuals",
    "section": "Accessibility concerns",
    "text": "Accessibility concerns\nIf your output is to be available online, it must be tested for accessibility and include (or link to) an Accessibility Statement. Full details are in the DALL Wiki.\n\nTry to create outputs with accessibility in mind; for example using the {shiny} template.\nDon’t test for accessibility too early; do it only once reasonably confident the non-text output is finalised."
  },
  {
    "objectID": "guidance/01-pre-pd/pre-pd-steps/05-initial-meeting-with-customer.html#general-considerations",
    "href": "guidance/01-pre-pd/pre-pd-steps/05-initial-meeting-with-customer.html#general-considerations",
    "title": "Meeting with the Customer",
    "section": "General considerations",
    "text": "General considerations\n\nIf you have any questions for the customer send them prior to the meeting. If you encountered any data issues, some examples could also help them prepare answers.\nIf the work is particularly urgent, more colleagues and resource can be assigned accordingly.\nSome customers will have busy schedules so it is important to setup the meeting in advance so the project can progress.\nYour critical friend should attend the meeting to share knowledge and be clear on the scope of the project.\nDocument any actions agreed at the meeting.\nSet up recurring follow up meetings at regular intervals.\nAny open questions following the meeting should be chased using either Teams or email."
  },
  {
    "objectID": "guidance/02-pd/pd-steps/02-customer-feedback.html#refining-the-problem-definition",
    "href": "guidance/02-pd/pd-steps/02-customer-feedback.html#refining-the-problem-definition",
    "title": "Customer feedback",
    "section": "Refining the problem definition",
    "text": "Refining the problem definition\n\nIt is important to repeat the problem definition loop again until the customer is happy to proceed.\nIn rare cases, this meeting may highlight that the initiative may not be possible in the way that the customer initially intended.\nUnless any suitable alternatives are identified, we may need to give up on the project.\nIn this case, you must discuss your concerns with the DALL team manager before communicating this with the customer."
  },
  {
    "objectID": "guidance/02-pd/pd-steps/03-refine-pd.html#tips-for-writing-a-problem-definition",
    "href": "guidance/02-pd/pd-steps/03-refine-pd.html#tips-for-writing-a-problem-definition",
    "title": "Refine the problem definition",
    "section": "Tips for writing a problem definition",
    "text": "Tips for writing a problem definition\n\nDuring this process, you will need to tell them what sensitive data you are using.\nExplain how you intend to use the data.\nFor sensitive data containing personally identifiable information (PII), you will need to get approval from the Information Governance (IG) team before you start the work.\nEnsure the customer understands the aim by providing plenty of detail.\nHonestly assess time frames and relay these to the customer.\nYour critical friend and DALL team manager can assist with this process.\n\nFor more information, please see the DALL Wiki.\nThroughout the process, your critical friend will review the problem definition to check:\n\nif you understood the customer’s request correctly\nif your plans to solve the problems are appropriate\n\nAlso, make sure to share the problem definition with the customer(s) to get their feedback, understand their business needs, and make sure you have addressed all their requests."
  },
  {
    "objectID": "guidance/04-wrap-up/wrap-up-steps/01-clean-up.html#general-considerations",
    "href": "guidance/04-wrap-up/wrap-up-steps/01-clean-up.html#general-considerations",
    "title": "Wrap up and clean your code",
    "section": "General considerations",
    "text": "General considerations\n\nWork with the mindset that all work could be make public.\nSmall and precise commits are better than large ones.\nMaintain a clear project structure with clean scripts.\nSeparate data, scripts and output into their own sub-directories.\nMove custom functions into their own script.\nDelete stale git branches or merge them into main.\nStreamline the codebase.\nCreate a config file with key parameters and filters that are referenced throughout the project.\nIdentify bottlenecks and optimise the code.\nEnsure key tables within the database are available to the wider team.\nRemove personal tables from your own database schema after the project has concluded.\nThis is especially important for personally identifiable information.\n\nPlease refer to the DALL OneNote Wiki for more information on cleaning your code."
  },
  {
    "objectID": "guidance/01-pre-pd/pre-pd-steps/01-problem-identified.html#identifying-a-problem",
    "href": "guidance/01-pre-pd/pre-pd-steps/01-problem-identified.html#identifying-a-problem",
    "title": "Hypothesis identified",
    "section": "Identifying a problem",
    "text": "Identifying a problem\n\nProjects can be identified by Data Science team members, an internal NHSBSA customer, an external customer or recommendations from previous work.\nThey may also be identified through collaborative ideas sessions we run with other teams.\nCustomers can request work through nhsbsa.dall@nhs.net.\nThe Data Science Lead reviews projects for suitability in terms of scope and appropriateness, as well as any ethical implications. Projects may be triaged to other teams if not suitable.\nThe Data Science Lead normally assigns initiatives to team members.\nThe Data Science team can log ideas for projects on the ideas register."
  },
  {
    "objectID": "guidance/01-pre-pd/pre-pd-steps/02-assign-critical-friend.html#assigning-a-critical-friend",
    "href": "guidance/01-pre-pd/pre-pd-steps/02-assign-critical-friend.html#assigning-a-critical-friend",
    "title": "Assign a critical friend",
    "section": "Assigning a critical friend",
    "text": "Assigning a critical friend\n\nAssign a critical friend as early as possible.\nCritical friends are assigned by the project members or the Data Science Lead.\nThe Data Science Lead knows who is available and has valuable knowledge related to the upcoming project.\nIdeally, a critical friend should have prior experience with similar data or techniques.\nIf you are an area expert, you can choose a new team member as the critical friend so they can gain experience."
  },
  {
    "objectID": "guidance/01-pre-pd/pre-pd-steps/02-assign-critical-friend.html#working-with-your-critical-friend",
    "href": "guidance/01-pre-pd/pre-pd-steps/02-assign-critical-friend.html#working-with-your-critical-friend",
    "title": "Assign a critical friend",
    "section": "Working with your critical friend",
    "text": "Working with your critical friend\n\nThe critical friend will help by offering more insight and reviewing the project.\nRegularly meet with the critical friend to keep them updated.\nAssign work to review in small increments rather than all at once.\nRespect the critical friend’s diary and provide plenty of notice for work to be completed."
  },
  {
    "objectID": "guidance/01-pre-pd/pre-pd-steps/03-get-access-to-data.html#getting-access",
    "href": "guidance/01-pre-pd/pre-pd-steps/03-get-access-to-data.html#getting-access",
    "title": "Get access to the data",
    "section": "",
    "text": "If you don’t have access to the data already, you will need to request access.\nFor the DALP database:\n\naccess to the data needed for each initiative is issued by the DALL data holder and team manager\nsend an email to the data holder (copying in the team manager) specifiying the tables that you need access to\nthey will confirm and provide the data from the DALP Oracle database\n\nFor the DWCP database:\n\nyou must submit a service request to access the data that you require\nthe data may be provided direct from its source (if possible)\nif the data is in DWCP and you think it is worthwhile to have it in DALP, you can request this\n\nAdvice on how to request data from both sources can be found within the DALL OneNote Wiki."
  },
  {
    "objectID": "guidance/01-pre-pd/pre-pd-steps/06-create-initiative-resources.html#setting-up-the-resources",
    "href": "guidance/01-pre-pd/pre-pd-steps/06-create-initiative-resources.html#setting-up-the-resources",
    "title": "Create Initiative Resources",
    "section": "Setting up the resources",
    "text": "Setting up the resources\n\nIdentify the Data Science initiative number using JIRA.\nRemember to set the new GitHub repository to private.\nInvite all relevant Data Science team members as collaborators.\nCode should be written with the intention that it will be made public.\n\nFurther guidance on setting up resources can be found on the internal Data Science OneNote Wiki.\nAfter creating all resources, remember to:\n\nprovide regular updates to the JIRA ticket\nregularly push code additions and alterations to the GitHub repository"
  },
  {
    "objectID": "guidance/03-analysis/analysis-steps/02-analysis-modelling.html#tips-for-effective-analysis",
    "href": "guidance/03-analysis/analysis-steps/02-analysis-modelling.html#tips-for-effective-analysis",
    "title": "Analysis and modelling",
    "section": "Tips for effective analysis",
    "text": "Tips for effective analysis\n\nUse Github to handle version control, issues and code reviews.\nIn Python, use object-oriented programming (OOP) if you can.\nAlthough R does have support for OOP, it is not a big paradigm like in Python.\nBe careful of premature optimisation (called out by computer scientist Donald Knuth as ‘the root of all evil’ in software development).\nStart small, such as limiting data to some subset when developing metrics; this makes it easier to spot issues and allows for faster iteration of code.\nCheck the data looks correct at each step of transforming it; better still, know what it should be before writing and applying the step (the principle behind test driven development, TDD).\nWork on one component of output at a time, for example using {shiny} modules as in our template.\nFrequently run code as you write it, so any issues are encountered early and their source is clearer.\nYou may produce multiple smaller data sets to be used in your outputs; the considerations applying to base tables apply equally to these.\nThe final code should use RAP principles [LINK?]."
  },
  {
    "objectID": "guidance/04-wrap-up/wrap-up-steps/04-public-repository-alt-callouts.html",
    "href": "guidance/04-wrap-up/wrap-up-steps/04-public-repository-alt-callouts.html",
    "title": "Make the Repository Public (If Applicable) - Alternate Page",
    "section": "",
    "text": "Where applicable, we aim to generate reproducible code that is made open source. This is in line with guidance outlined in the government service manual.\nBefore making a repository public, please consider the risks.\nYou must check the repository does not compromise data security before making it public.\n\n\n\n\n\n\nGit commit history\n\n\n\n\n\nYour git commit history will be made public.\n\n\n\nGit messages can be changed but it is advised that you follow good commit practice.\n\n\nEach commit should be limited to a single change.\n\n\nCommit history can be removed but it is not recommended as we strive to be transparent.\n\n\n\n\n\n\n\n\n\n\n\nNever expose Personally Identifiable Information (PII) on GitHub\n\n\n\n\n\nMake sure that you never push Personally Identifiable Information (PII) to the GitHub repository.\n\n\n\nUse Git Leaks to reduce the risk of pushing PII to Git.\n\n\nNever write PII directly in R scripts.\n\n\nAlways reference PII from the database."
  },
  {
    "objectID": "guidance/01-pre-pd/pre-pd-steps/03-get-access-to-data.html#accessing-data",
    "href": "guidance/01-pre-pd/pre-pd-steps/03-get-access-to-data.html#accessing-data",
    "title": "Get access to the data and undertake initial understanding",
    "section": "",
    "text": "Temporary access must be requested via email or a service request and is subject to approval from the Data Science Lead, Database administrators and/or Data Asset Owner.\nShould sensitive data be needed, there may also be a requirement to engage with the Caldicott Guardian and/or Information Governance.\nAll data must be accessed and processed through the Data Science secure data environment.\nAlso request any relevant metadata, data dictionaries and data quality profiles."
  },
  {
    "objectID": "guidance/01-pre-pd/pre-pd-steps/03-get-access-to-data.html#initial-data-understanding",
    "href": "guidance/01-pre-pd/pre-pd-steps/03-get-access-to-data.html#initial-data-understanding",
    "title": "Get access to the data and undertake initial understanding",
    "section": "Initial data understanding",
    "text": "Initial data understanding\nInitial data understanding will help you to understand what is possible with the data including any anomalies, caveats, and quality issues.\n\nUse data documentation and identify data SMEs (within the services and/or central MI or Data Services) who will support you to understand what fields are available and their suitability.\nInitial exploratory data analysis will also help you to get a better feel for the data in terms of its quality and suitability and what data preparation might be required.\nShould documentation not exist, you may wish to record:\n\na summary of what each column represents\nthe number of missing records\nthe number of errors\nif applicable, the logic used to calculate the column\n\n\nCombined with business understanding, this initial analysis will help to:\n\ninform the problem definition (PD)\ninform conversations with the customer of what is possible and not possible to achieve during the initiative\nidentify other interesting research questions with the customer and explore if these are worth pursuing"
  },
  {
    "objectID": "guidance/01-pre-pd/pre-pd-steps/03-get-access-to-data.html#tips-for-gaining-understanding",
    "href": "guidance/01-pre-pd/pre-pd-steps/03-get-access-to-data.html#tips-for-gaining-understanding",
    "title": "Get access to the data and undertake initial understanding",
    "section": "Tips for gaining understanding",
    "text": "Tips for gaining understanding\n\nIt is a good idea to keep a list of all questions you have with the data.\nFor further insight, check JIRA and ask around to see if anyone has explored this data before.\nThe Critical friend and team manager can also provide assistance.\n\nFinally, it is possible that other analytical teams within the BSA have worked with the data before and may be able to offer insight.\n\nThe Management Information (MI) team produce a range of dashboards in a variety of areas.\nThe External Reporting team manage the ePACT2 (prescriptions), eDEN (dental) and eOPS (ophthalmic) systems."
  },
  {
    "objectID": "guidance/02-pd/pd-steps/01-data-business-understanding.html#what-is-a-problem-definition",
    "href": "guidance/02-pd/pd-steps/01-data-business-understanding.html#what-is-a-problem-definition",
    "title": "Writing the Problem Definition",
    "section": "",
    "text": "An agreement between the Data Scientists and the customer(s).\nIt defines a set of goals that everyone agrees on before work begins.\nIt ensures there is a common understanding.\nIt should be stored within the SharePoint folder for reference.\n\nAs part of writing the problem definition, you will need to include:\n\nsome background information about the area\nwhat it is that needs to be solved\nhow we intend to solve the problem\nany limitations and caveats, if they exist\nwhat tools and data we will use\nany information governance and ethical considerations, if they are applicable"
  },
  {
    "objectID": "guidance/02-pd/pd-steps/01-data-business-understanding.html#tips-for-writing-a-problem-definition",
    "href": "guidance/02-pd/pd-steps/01-data-business-understanding.html#tips-for-writing-a-problem-definition",
    "title": "Writing the Problem Definition",
    "section": "Tips for writing a problem definition",
    "text": "Tips for writing a problem definition\n\nDuring this process, you will need to tell them what sensitive data you are using.\nExplain how you intend to use the data.\nFor sensitive data containing personally identifiable information (PII), you will need to get approval from the Information Governance (IG) team before you start the work.\nEnsure the customer understands the aim by providing plenty of detail.\nHonestly assess time frames and relay these to the customer.\nYour critical friend and DALL team manager can assist with this process.\n\nFor more information, please see the internal Data Science One Note Wiki.\nThroughout the process, your critical friend will review the problem definition to check:\n\nif you understood the customer’s request correctly\nif your plans to solve the problems are appropriate\n\nAlso, make sure to share the problem definition with the customer(s) to get their feedback, understand their business needs, and make sure you have addressed all their requests."
  },
  {
    "objectID": "guidance/01-pre-pd/pre-pd-steps/01-hypothesis-identified.html",
    "href": "guidance/01-pre-pd/pre-pd-steps/01-hypothesis-identified.html",
    "title": "Hypothesis identified",
    "section": "",
    "text": "The Data Science team deliver actionable insights from data through innovation, experimentation and collaboration. The insights drive policy, decision making and efficiencies across the NHSBSA and wider health and social care system. Projects typically align to the following areas:"
  },
  {
    "objectID": "guidance/01-pre-pd/pre-pd-steps/01-hypothesis-identified.html#identifying-a-problem",
    "href": "guidance/01-pre-pd/pre-pd-steps/01-hypothesis-identified.html#identifying-a-problem",
    "title": "Hypothesis identified",
    "section": "Identifying a problem",
    "text": "Identifying a problem\n\nProjects can be identified by Data Science team members, an internal NHSBSA customer, an external customer or recommendations from previous work.\nThey may also be identified through collaborative ideas sessions we run with other teams.\nCustomers can request work through nhsbsa.dall@nhs.net.\nThe Data Science Lead reviews projects for suitability in terms of scope and appropriateness, as well as any ethical implications. Projects may be triaged to other teams if not suitable.\nThe Data Science Lead normally assigns initiatives to team members.\nThe Data Science team can log ideas for projects on the ideas register."
  },
  {
    "objectID": "guidance/01-pre-pd/pre-pd-steps/04-desk-research.html",
    "href": "guidance/01-pre-pd/pre-pd-steps/04-desk-research.html",
    "title": "Desk Research and Business Understanding",
    "section": "",
    "text": "Desk research and business understanding will help you understand what is possible for the initiative and ensure the work is not duplicated.\nTo assist your work you could use:\n\nJIRA as a reference to previous initiatives\nreports and other outputs from previous initiatives in the project folders\nother team members, business SMEs and your critical friend to help develop business knowledge\ninternal and external online resources which may describe the service, policy or other aspects relevant to understanding context\nexternally or internal published statistics, data or dashboards relevant to the subject. They may also later be a source for validation and ensuring coherence.\nresources to support with methodology such as Github repositories, Stack Overflow, Medium and research papers\nDataCamp to complete relevant courses\n\nThis is a guide of what is possible and is dependent on the current project’s requirements."
  },
  {
    "objectID": "guidance/02-pd/pd-steps/01-writing-problem-definition.html",
    "href": "guidance/02-pd/pd-steps/01-writing-problem-definition.html",
    "title": "Writing the Problem Definition",
    "section": "",
    "text": "An agreement between the Data Scientists and the customer(s).\nIt defines a set of goals that everyone agrees on before work begins.\nIt ensures there is a common understanding.\nIt should be stored within the SharePoint folder for reference.\n\nAs part of writing the problem definition, you will need to include:\n\nsome background information about the area\nwhat it is that needs to be solved\nhow we intend to solve the problem\nany limitations and caveats, if they exist\nwhat tools and data we will use\nany information governance and ethical considerations, if they are applicable"
  },
  {
    "objectID": "guidance/02-pd/pd-steps/01-writing-problem-definition.html#what-is-a-problem-definition",
    "href": "guidance/02-pd/pd-steps/01-writing-problem-definition.html#what-is-a-problem-definition",
    "title": "Writing the Problem Definition",
    "section": "",
    "text": "An agreement between the Data Scientists and the customer(s).\nIt defines a set of goals that everyone agrees on before work begins.\nIt ensures there is a common understanding.\nIt should be stored within the SharePoint folder for reference.\n\nAs part of writing the problem definition, you will need to include:\n\nsome background information about the area\nwhat it is that needs to be solved\nhow we intend to solve the problem\nany limitations and caveats, if they exist\nwhat tools and data we will use\nany information governance and ethical considerations, if they are applicable"
  },
  {
    "objectID": "guidance/02-pd/pd-steps/01-writing-problem-definition.html#tips-for-writing-a-problem-definition",
    "href": "guidance/02-pd/pd-steps/01-writing-problem-definition.html#tips-for-writing-a-problem-definition",
    "title": "Writing the Problem Definition",
    "section": "Tips for writing a problem definition",
    "text": "Tips for writing a problem definition\n\nDuring this process, you will need to tell them what sensitive data you are using.\nExplain how you intend to use the data.\nFor sensitive data containing personally identifiable information (PII), you will need to get approval from the Information Governance (IG) team before you start the work.\nEnsure the customer understands the aim by providing plenty of detail.\nHonestly assess time frames and relay these to the customer.\nYour critical friend and DALL team manager can assist with this process.\n\nFor more information, please see the internal Data Science One Note Wiki.\nThroughout the process, your critical friend will review the problem definition to check:\n\nif you understood the customer’s request correctly\nif your plans to solve the problems are appropriate\n\nAlso, make sure to share the problem definition with the customer(s) to get their feedback, understand their business needs, and make sure you have addressed all their requests."
  }
]