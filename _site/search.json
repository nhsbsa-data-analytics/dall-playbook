[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NHSBSA DALL Playbook",
    "section": "",
    "text": "Add Text Here"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This website has been developed by the NHSBSA Official Statistics team to provide guidance and processes for producing new and existing Official Statistics publications, including those with National Statistics designation."
  },
  {
    "objectID": "guidance/process/index.html",
    "href": "guidance/process/index.html",
    "title": "Process for producing a publication",
    "section": "",
    "text": "Each publication is assigned a responsible statistician. This is the person who will lead on the development and maintenance of the publication and be the main point of contact for queries about the statistics. Our current publications and responsible statisticians are:\n\n\n\nPublication\nLead statistician\n\n\n\n\nDependency forming medicines\nKirsty Gray\n\n\nGeneral pharmaceutical services\nKirsty Gray\n\n\nHormone replacement therapy\nGraham Platten\n\n\nMedicines used in mental health — annual\nKirsty Gray\n\n\nMedicines used in mental health — quarterly\nGrace Libby\n\n\nPrescribing costs in hospitals and the community\nGraham Platten\n\n\nPrescribing for diabetes\nKirsty Gray\n\n\nPrescription cost analysis\nGraham Platten"
  },
  {
    "objectID": "guidance/process/index.html#high-level-process",
    "href": "guidance/process/index.html#high-level-process",
    "title": "Process for producing a publication",
    "section": "High level process",
    "text": "High level process\nEach publication will go through the below stages before release. Each stage is expanded upon further on in this guidance.\n\n\n\n\n\nflowchart LR\n  A([start]) --> B[Review content]\n  B --> C[planning]\n  C --> D[Production]\n  D --> E[QA]\n  E --> F[Sign off]\n  F --> D & G(Publish)\n\n\n\n\n\n\nFigure 1: High level publication production process"
  },
  {
    "objectID": "guidance/process/index.html#reviewing-content",
    "href": "guidance/process/index.html#reviewing-content",
    "title": "Process for producing a publication",
    "section": "Reviewing content",
    "text": "Reviewing content\nIn line with the Code of Practice for Statistics, particularly principles V1: Relevance to users and V4: Innovation and improvement, we should always be reviewing the content of our statistics to make sure that they provide public value and meet our user’s needs.\n\nResearch, policy developments and media interests\nAs we begin the production for a new release of a publication the responsible statistician should make sure to familiarise themselves with the content of the publication including the data sources, and their quality, and any policy context/background that we have. A lot can change in the time between releases especially for our prescribing and medicines publications, keeping up to date with policy developments, the release of new prescribing guidance, and what has been in the media is essential. This research should be documented so that it can easily be referenced in the future.\n\n\nUser and stakeholder feedback\nFeedback collected through our continuous surveys, provided via email, or any other route should be reviewed and help guide any changes and improvements that we want to make to the statistics. This could include requests for new analyses, feedback on usability and clarity, or even feedback that an analysis is no longer needed. The views of interested stakeholders should also be regularly sought to make sure that the statistics fulfil their intended purpose. For example, colleagues at the Department for Health and Social Care (DHSC) and NHS England.\n\n\nNew and emerging services\nNew services may become available that would be of interest to include in our publications, for example, a new advanced service for pharmacy contractors or a new service launched by the NHSBSA. Including data and analyses on these new services may provide new insights and increase the value of our statistics.\n\n\nCriteria and SME engagement\nThe criteria for our publications should also be reviewed to make sure that they are still relevant. For example, for publications in specific clinical areas such as Medicines Used in Mental Health and Prescribing for Diabetes the drug lists used should be reviewed and where appropriate the views of clinical colleagues should be sought. New products may have come to market or the BNF classifications of some existing products changed1, We need to make sure that the drug lists that we use are up to date, comprehensive and clinically approved. Subject matter experts (SMEs) should be engaged here to provide guidance.\n\n\nData quality issues\nAs with any data set mistakes can and do happen in our data. To make sure that we are aware of any data quality issues that may impact our statistics the data quality incident register maintained by the Data Governance team should be reviewed. Where a data quality issue is identified this should be noted and steps taken to correct the data or communicated to users what the issue is.\n\n\nIdentified improvements and opportunities\nWe as statisticians often know of improvements that we can make to our outputs to make them more relevant to users or may have seen demonstrations of best practice from across the UK statistical system. During publication retrospectives ideas for improvements to take forward are logged and recorded as part of each publication’s wiki. These ideas should be reviewed and where needed exploratory analysis carried out and documented.\n\n\nMethodological changes\nWe should always be reviewing the methodologies that we use in our statistics to make sure that they are recognised best practice and coherent with other statistics across the health and social care sector. If an improvement to a methodology is identified, for example, calculating standardised population rates or assigning patients to a particular lower-layer super output area (LSOA), then this improvement should be documented alongside any impacts that it may have on the statistics. There is not a dedicated methods group at the NHSBSA, however, other statisticians and analysts in the Data Analytics Learning Lab (DALL) and External Reporting teams should be invited to review the new methodology and discussions held on possible alternatives. Where practicable, all statisticians, data scientists, and analysts across the NHSBSA should be using standardised methods for analysis.\nWhere a new method is statistically innovative or a wholesale change from what has been used before it may be advised to consult the Methodology Advisory Service(MAS) at the Office for National Statistics (ONS).\n\n\nUser engagement and pre-announcing changes\nOnce a review of content has been completed all changes that have been decided to be carried forward for the new release should be documented in full, including additions/removal of content. The impacts of any changes should also be explored and documented, for example, if a methodological change is being made or if a data source is changing. The views of the NHSBSA Lead Official for Statistics and/or Statistical Publication Lead should then be sought on the proposed changes to provide guidance.\nAny changes that we are making to the statistics must be pre-announced on the NHSBSA website well in advance of the release of the statistics (at least 4 weeks in advance). Where practicable, users views should be sought on the proposed changes, but were these changes have been guided by user feedback already this may not be necessary.\nWhere large methodological changes have taken place or where the underlying data source for the statistics has changed and materially impacted new or historical figures users views must be sought before the change is implemented — particularly for National Statistics publications. User engagement can be done through a variety of methods, including:\n\ndedicated engagement events\npublic consultations\nblogs detailing the changes and impacts\nother activities\n\nUser engagement is often done in conjunction with other teams throughout the organisation such as Communications and Customer Insight. These teams should be consulted to help support our engagement activities.\n\n\nchecklist\n\ndo you know what analyses are in the statistics and their data source?\nare all data sources still available?\nwhat policy developments have their been?\nhave there been any FOI/data/PQ requests that show there is a gap in the statistics?\nare new services/areas for analysis available? Have any services stopped?\nhave users provided any feedback that can be acted upon?\nhave there been any data quality incidents that would impact the statistics?\nwhat areas did we identify for improvement from the previous release’s debrief?\nhave you documented the review process and formulated an improvement plan for the release"
  },
  {
    "objectID": "guidance/process/index.html#user-engagement-and-pre-announcing-changes",
    "href": "guidance/process/index.html#user-engagement-and-pre-announcing-changes",
    "title": "Process for producing a publication",
    "section": "User engagement and pre-announcing changes",
    "text": "User engagement and pre-announcing changes\nOnce\n\nchecklist\n\ndo you know what analyses are in the statistics and their data source?\nare all data sources still available?\nwhat policy developments have their been?\nare new services/areas for analysis available? Have any services stopped?\nhave users provided any feedback that can be acted upon?\nwhat areas did we identify for improvement from the previous release’s debrief?\nhave you documented the review process and formulated an improvement plan for the release"
  },
  {
    "objectID": "guidance/process/index.html#planning",
    "href": "guidance/process/index.html#planning",
    "title": "Process for producing a publication",
    "section": "Planning",
    "text": "Planning\nOnce"
  },
  {
    "objectID": "index.html#high-level-process",
    "href": "index.html#high-level-process",
    "title": "NHSBSA DALL Playbook",
    "section": "High level process",
    "text": "High level process\nAdd Text Here\n\n\n\n\n\n\n\nG\n\n \n\ncluster_analysis_loop\n\n       03: Analysis and modelling  \n\ncluster_dummy_c\n\n \n\ncluster_review\n\n       04: Initiative review  \n\ncluster_dummy_d\n\n \n\ncluster_pre_pd\n\n       01: Project setup  \n\ncluster_dummy_a\n\n \n\ncluster_pd_loop\n\n       02: Problem definition  \n\ncluster_dummy_b\n\n     \n\na1\n\n       Problem Identified      \n\na2\n\n       Assign Critical Friend   \n\na1-&gt;a2\n\n    \n\nb1\n\n       Write Problem Definition   \n\na1-&gt;b1\n\n  Items Created   \n\na3\n\n       Initial Research and Look at Previous Iterations   \n\na2-&gt;a3\n\n    \n\na4\n\n       Initial Meeting with Customer   \n\na3-&gt;a4\n\n    \n\na5\n\n       Get Access to Data   \n\na4-&gt;a5\n\n    \n\na6\n\n       Create JIRA, SharePoint, Git and Others   \n\na5-&gt;a6\n\n        \n\nb2\n\n       Data and Business Understanding   \n\nb1-&gt;b2\n\n    \n\nc1\n\n       Data Preparation   \n\nb1-&gt;c1\n\n  Sign Off From Customer   \n\nb3\n\n       Further Research   \n\nb2-&gt;b3\n\n    \n\nb4\n\n       Meet with Customer   \n\nb3-&gt;b4\n\n    \n\nb4-&gt;b1\n\n        \n\nc2\n\n       Analysis/Modelling   \n\nc1-&gt;c2\n\n    \n\nd1\n\n       Final Meeting with Customer   \n\nc1-&gt;d1\n\n  Work Completed   \n\nc3\n\n       Reporting and Visuals   \n\nc2-&gt;c3\n\n    \n\nc4\n\n       Review and Test   \n\nc3-&gt;c4\n\n    \n\nc5\n\n       Critical Friend to Review   \n\nc4-&gt;c5\n\n    \n\nc6\n\n       Customer Review   \n\nc5-&gt;c6\n\n    \n\nc6-&gt;c1\n\n      \n\nzz\n\nzz    \n\nd2\n\n       Wrap Up and  Clean Code   \n\nd1-&gt;d2\n\n    \n\nd3\n\n       Write Documentation   \n\nd2-&gt;d3\n\n    \n\nd4\n\n       Add to Wiki   \n\nd3-&gt;d4\n\n    \n\nd5\n\n       Make Repo Public If Applicable   \n\nd4-&gt;d5\n\n    \n\nd6\n\n       Document Any Further Work That Can Be Done   \n\nd5-&gt;d6\n\n   \n\n\n\n\n\n\n\n\n\n\n  High level process  \n\npre_pd\n\n 01: Project setup   \n\npd_loop\n\n 02: Problem definition   \n\npre_pd-&gt;pd_loop\n\n  Items created   \n\nanalysis_loop\n\n 03: Analysis and modelling   \n\npd_loop-&gt;analysis_loop\n\n  Sign off from customer   \n\nreview\n\n 04: Initiative review   \n\nanalysis_loop-&gt;review\n\n  Work completed  \n\n\n\n\n\n\n\n\n\n\n%%{\n  init: {\n    'theme': 'neutral',\n    'themeVariables': {\n      'primaryColor': '#BB2528',\n      'primaryTextColor': '#fff',\n      'primaryBorderColor': '#7C0000',\n      'lineColor': '#F8B229',\n      'secondaryColor': '#006100',\n      'tertiaryColor': '#fff',\n      'fontSize': '16px'\n    }\n  }\n}%%\n\nflowchart LR\n  1A[\"Problem Identified\"] \n  1B[\"Assign Critical Friend\"]\n  1C[\"Initial Research and Look at Previous Iterations\"]\n  1D[\"Initial Meeting with Customer\"]\n  1E[\"Get Access to Data\"]\n  1F[\"Create JIRA, SharePoint, Git and Others\"]\n    \n  1G[\"Write Problem \n    Definition\"] \n  1H[\"Data and \n    Business \n    Understanding\"] \n  1I[\"Further \n    Research\"] \n  1J[\"Meet with \n    Customer\"]\n  \n  2A[\"Data Preparation\"]\n  2B[\"Analysis/Modelling\"] \n  2C[\"Reporting and Visuals\"] \n  2D[\"Review and Test\"] \n  2E[\"Critical Friend to Review\"] \n  2F[\"Customer Review\"]\n  \n  3A[\"Final Meeting with Customer\"]\n  3B[\"Wrap Up and  Clean Code\"]\n  3C[\"Write Documentation\"]\n  3D[\"Add to Wiki\"]\n  3E[\"Make Repo Public If Applicable\"]\n  3F[\"Document Any Further Work That Can Be Done\"]\n\n  subgraph pre_pd[\"01: Project Setup\"]\n  direction TB\n    1A --&gt; 1B\n    1B --&gt; 1C\n    1C --&gt; 1D\n    1D --&gt; 1E\n    1E --&gt; 1F\n  end\n  \n  subgraph pd_loop[\"02: Problem Definition\"]\n  direction TB\n    1G --&gt; 1H --&gt; 1I --&gt; 1J --&gt; 1G\n  end\n  \n  subgraph analysis_loop[\"03: Analysis/Modelling\"]\n  direction TB\n    2A --&gt; 2B --&gt; 2C --&gt; 2D --&gt; 2E --&gt; 2F --&gt; 2A\n  end\n  \n  subgraph review[\"04: Initiative Review\"]\n  direction TB\n    3A --&gt; 3B\n    3B --&gt; 3C\n    3C --&gt; 3D\n    3D --&gt; 3E\n    3E --&gt; 3F\n  end\n\n  pre_pd --&gt;|Items &lt;BR&gt; Created| pd_loop\n  pd_loop --&gt;|Sign Off From &lt;BR&gt; Customer| analysis_loop\n  analysis_loop --&gt;|Work &lt;BR&gt; Completed| review\n  \n  click 1B \"http://www.github.com\"\n\n\nFigure 1: High level process"
  },
  {
    "objectID": "index.html#guidance",
    "href": "index.html#guidance",
    "title": "NHSBSA DALL Playbook",
    "section": "Guidance",
    "text": "Guidance"
  },
  {
    "objectID": "guidance/reviewing-content/index.html",
    "href": "guidance/reviewing-content/index.html",
    "title": "Reviewing content",
    "section": "",
    "text": "In line with the Code of Practice for Statistics, particularly principles V1: Relevance to users and V4: Innovation and improvement, we should always be reviewing the content of our statistics to make sure that they provide public value and meet our user’s needs."
  },
  {
    "objectID": "guidance/reviewing-content/index.html#reviewing-content",
    "href": "guidance/reviewing-content/index.html#reviewing-content",
    "title": "Process for producing a publication",
    "section": "Reviewing content",
    "text": "Reviewing content\nIn line with the Code of Practice for Statistics, particularly principles V1: Relevance to users and V4: Innovation and improvement, we should always be reviewing the content of our statistics to make sure that they provide public value and meet our user’s needs.\n\nResearch, policy developments and media interests\nAs we begin the production for a new release of a publication the responsible statistician should make sure to familiarise themselves with the content of the publication including the data sources, and their quality, and any policy context/background that we have. A lot can change in the time between releases especially for our prescribing and medicines publications, keeping up to date with policy developments, the release of new prescribing guidance, and what has been in the media is essential. This research should be documented so that it can easily be referenced in the future.\n\n\nUser and stakeholder feedback\nFeedback collected through our continuous surveys, provided via email, or any other route should be reviewed and help guide any changes and improvements that we want to make to the statistics. This could include requests for new analyses, feedback on usability and clarity, or even feedback that an analysis is no longer needed. The views of interested stakeholders should also be regularly sought to make sure that the statistics fulfil their intended purpose. For example, colleagues at the Department for Health and Social Care (DHSC) and NHS England.\n\n\nNew and emerging services\nNew services may become available that would be of interest to include in our publications, for example, a new advanced service for pharmacy contractors or a new service launched by the NHSBSA. Including data and analyses on these new services may provide new insights and increase the value of our statistics.\n\n\nCriteria and SME engagement\nThe criteria for our publications should also be reviewed to make sure that they are still relevant. For example, for publications in specific clinical areas such as Medicines Used in Mental Health and Prescribing for Diabetes the drug lists used should be reviewed and where appropriate the views of clinical colleagues should be sought. New products may have come to market or the BNF classifications of some existing products changed1, We need to make sure that the drug lists that we use are up to date, comprehensive and clinically approved. Subject matter experts (SMEs) should be engaged here to provide guidance.\n\n\nData quality issues\nAs with any data set mistakes can and do happen in our data. To make sure that we are aware of any data quality issues that may impact our statistics the data quality incident register maintained by the Data Governance team should be reviewed. Where a data quality issue is identified this should be noted and steps taken to correct the data or communicated to users what the issue is.\n\n\nIdentified improvements and opportunities\nWe as statisticians often know of improvements that we can make to our outputs to make them more relevant to users or may have seen demonstrations of best practice from across the UK statistical system. During publication retrospectives ideas for improvements to take forward are logged and recorded as part of each publication’s wiki. These ideas should be reviewed and where needed exploratory analysis carried out and documented.\n\n\nMethodological changes\nWe should always be reviewing the methodologies that we use in our statistics to make sure that they are recognised best practice and coherent with other statistics across the health and social care sector. If an improvement to a methodology is identified, for example, calculating standardised population rates or assigning patients to a particular lower-layer super output area (LSOA), then this improvement should be documented alongside any impacts that it may have on the statistics. There is not a dedicated methods group at the NHSBSA, however, other statisticians and analysts in the Data Analytics Learning Lab (DALL) and External Reporting teams should be invited to review the new methodology and discussions held on possible alternatives. Where practicable, all statisticians, data scientists, and analysts across the NHSBSA should be using standardised methods for analysis.\nWhere a new method is statistically innovative or a wholesale change from what has been used before it may be advised to consult the Methodology Advisory Service(MAS) at the Office for National Statistics (ONS).\n\n\nUser engagement and pre-announcing changes\nOnce a review of content has been completed all changes that have been decided to be carried forward for the new release should be documented in full, including additions/removal of content. The impacts of any changes should also be explored and documented, for example, if a methodological change is being made or if a data source is changing. The views of the NHSBSA Lead Official for Statistics and/or Statistical Publication Lead should then be sought on the proposed changes to provide guidance.\nAny changes that we are making to the statistics must be pre-announced on the NHSBSA website well in advance of the release of the statistics (at least 4 weeks in advance). Where practicable, users views should be sought on the proposed changes, but were these changes have been guided by user feedback already this may not be necessary.\nWhere large methodological changes have taken place or where the underlying data source for the statistics has changed and materially impacted new or historical figures users views must be sought before the change is implemented — particularly for National Statistics publications. User engagement can be done through a variety of methods, including:\n\ndedicated engagement events\npublic consultations\nblogs detailing the changes and impacts\nother activities\n\nUser engagement is often done in conjunction with other teams throughout the organisation such as Communications and Customer Insight. These teams should be consulted to help support our engagement activities.\n\n\nchecklist\n\ndo you know what analyses are in the statistics and their data source?\nare all data sources still available?\nwhat policy developments have their been?\nhave there been any FOI/data/PQ requests that show there is a gap in the statistics?\nare new services/areas for analysis available? Have any services stopped?\nhave users provided any feedback that can be acted upon?\nhave there been any data quality incidents that would impact the statistics?\nwhat areas did we identify for improvement from the previous release’s debrief?\nhave you documented the review process and formulated an improvement plan for the release"
  },
  {
    "objectID": "guidance/reviewing-content/index.html#planning",
    "href": "guidance/reviewing-content/index.html#planning",
    "title": "Process for producing a publication",
    "section": "Planning",
    "text": "Planning\nOnce"
  },
  {
    "objectID": "guidance/responsible-statisticians/index.html",
    "href": "guidance/responsible-statisticians/index.html",
    "title": "Responsible statisticians",
    "section": "",
    "text": "Publication\nLead statistician\n\n\n\n\nDependency forming medicines\nKirsty Gray\n\n\nGeneral pharmaceutical services\nKirsty Gray\n\n\nHormone replacement therapy\nGraham Platten\n\n\nMedicines used in mental health — annual\nKirsty Gray\n\n\nMedicines used in mental health — quarterly\nGrace Libby\n\n\nPrescribing costs in hospitals and the community\nGraham Platten\n\n\nPrescribing for diabetes\nKirsty Gray\n\n\nPrescription cost analysis\nGraham Platten\n\n\n\nResponsible statisticians are expected to be subject matter experts (SMEs) for the topic areas that they cover."
  },
  {
    "objectID": "guidance/planning/index.html",
    "href": "guidance/planning/index.html",
    "title": "Planning",
    "section": "",
    "text": "Each publication production cycle needs to be planned out in order to make sure that we have all of the outputs ready in time to be fully assured and each team member knows what they are responsible for delivering — every publication is a collaborative effort."
  },
  {
    "objectID": "guidance/pra/index.html",
    "href": "guidance/pra/index.html",
    "title": "Pre-release access (PRA)",
    "section": "",
    "text": "Guidance coming soon"
  },
  {
    "objectID": "guidance/production/index.html",
    "href": "guidance/production/index.html",
    "title": "Production",
    "section": "",
    "text": "Guidance coming soon"
  },
  {
    "objectID": "guidance/publish/index.html",
    "href": "guidance/publish/index.html",
    "title": "Publishing content",
    "section": "",
    "text": "Guidance coming soon"
  },
  {
    "objectID": "guidance/qa/index.html",
    "href": "guidance/qa/index.html",
    "title": "Quality assurance",
    "section": "",
    "text": "Guidance coming soon"
  },
  {
    "objectID": "guidance/sign-off/index.html",
    "href": "guidance/sign-off/index.html",
    "title": "Sign off",
    "section": "",
    "text": "Guidance coming soon"
  },
  {
    "objectID": "guidance/planning/index.html#planning-meeting",
    "href": "guidance/planning/index.html#planning-meeting",
    "title": "Planning",
    "section": "Planning meeting",
    "text": "Planning meeting\nA meeting should be organised to bring together the team to discuss the tasks required and assign them to a team member. The responsible statistician for the publication should arrange this meeting for a time when everyone is available. This meeting can be in-person, virtual, or hybrid."
  },
  {
    "objectID": "guidance/planning/index.html#github-issues",
    "href": "guidance/planning/index.html#github-issues",
    "title": "Planning",
    "section": "GitHub issues",
    "text": "GitHub issues\nWe currently use GitHub issues to describe, assign, and track tasks for a publication. If more suitable software is available for a particular publication or project then this should be used instead (for example, JIRA). Issues should be created in the pipeline repository of the publication and milestones used to group related tasks together and assign due dates.\nAs much detail as possible should be put into the issue about the task to allow the assigned person to be able to pick it up and complete it independently."
  },
  {
    "objectID": "guidance/planning/index.html#assigning-issues",
    "href": "guidance/planning/index.html#assigning-issues",
    "title": "Planning",
    "section": "Assigning issues",
    "text": "Assigning issues"
  },
  {
    "objectID": "guidance/reviewing-content/index.html#research-policy-developments-and-media-interests",
    "href": "guidance/reviewing-content/index.html#research-policy-developments-and-media-interests",
    "title": "Reviewing content",
    "section": "Research, policy developments and media interests",
    "text": "Research, policy developments and media interests\nAs we begin the production for a new release of a publication the responsible statistician should make sure to familiarise themselves with the content of the publication including the data sources, and their quality, and any policy context/background that we have. A lot can change in the time between releases especially for our prescribing and medicines publications, keeping up to date with policy developments, the release of new prescribing guidance, and what has been in the media is essential. This research should be documented so that it can easily be referenced in the future."
  },
  {
    "objectID": "guidance/reviewing-content/index.html#user-and-stakeholder-feedback",
    "href": "guidance/reviewing-content/index.html#user-and-stakeholder-feedback",
    "title": "Reviewing content",
    "section": "User and stakeholder feedback",
    "text": "User and stakeholder feedback\nFeedback collected through our continuous surveys, provided via email, or any other route should be reviewed and help guide any changes and improvements that we want to make to the statistics. This could include requests for new analyses, feedback on usability and clarity, or even feedback that an analysis is no longer needed. The views of interested stakeholders should also be regularly sought to make sure that the statistics fulfil their intended purpose. For example, colleagues at the Department for Health and Social Care (DHSC) and NHS England."
  },
  {
    "objectID": "guidance/reviewing-content/index.html#new-and-emerging-services",
    "href": "guidance/reviewing-content/index.html#new-and-emerging-services",
    "title": "Reviewing content",
    "section": "New and emerging services",
    "text": "New and emerging services\nNew services may become available that would be of interest to include in our publications, for example, a new advanced service for pharmacy contractors or a new service launched by the NHSBSA. Including data and analyses on these new services may provide new insights and increase the value of our statistics."
  },
  {
    "objectID": "guidance/reviewing-content/index.html#criteria-and-sme-engagement",
    "href": "guidance/reviewing-content/index.html#criteria-and-sme-engagement",
    "title": "Reviewing content",
    "section": "Criteria and SME engagement",
    "text": "Criteria and SME engagement\nThe criteria for our publications should also be reviewed to make sure that they are still relevant. For example, for publications in specific clinical areas such as Medicines Used in Mental Health and Prescribing for Diabetes the drug lists used should be reviewed and where appropriate the views of clinical colleagues should be sought. New products may have come to market or the BNF classifications of some existing products changed1, We need to make sure that the drug lists that we use are up to date, comprehensive and clinically approved. Subject matter experts (SMEs) should be engaged here to provide guidance."
  },
  {
    "objectID": "guidance/reviewing-content/index.html#data-quality-issues",
    "href": "guidance/reviewing-content/index.html#data-quality-issues",
    "title": "Reviewing content",
    "section": "Data quality issues",
    "text": "Data quality issues\nAs with any data set mistakes can and do happen in our data. To make sure that we are aware of any data quality issues that may impact our statistics the data quality incident register maintained by the Data Governance team should be reviewed. Where a data quality issue is identified this should be noted and steps taken to correct the data or communicated to users what the issue is."
  },
  {
    "objectID": "guidance/reviewing-content/index.html#identified-improvements-and-opportunities",
    "href": "guidance/reviewing-content/index.html#identified-improvements-and-opportunities",
    "title": "Reviewing content",
    "section": "Identified improvements and opportunities",
    "text": "Identified improvements and opportunities\nWe as statisticians often know of improvements that we can make to our outputs to make them more relevant to users or may have seen demonstrations of best practice from across the UK statistical system. During publication retrospectives ideas for improvements to take forward are logged and recorded as part of each publication’s wiki. These ideas should be reviewed and where needed exploratory analysis carried out and documented."
  },
  {
    "objectID": "guidance/reviewing-content/index.html#methodological-changes",
    "href": "guidance/reviewing-content/index.html#methodological-changes",
    "title": "Reviewing content",
    "section": "Methodological changes",
    "text": "Methodological changes\nWe should always be reviewing the methodologies that we use in our statistics to make sure that they are recognised best practice and coherent with other statistics across the health and social care sector. If an improvement to a methodology is identified, for example, calculating standardised population rates or assigning patients to a particular lower-layer super output area (LSOA), then this improvement should be documented alongside any impacts that it may have on the statistics. There is not a dedicated methods group at the NHSBSA, however, other statisticians and analysts in the Data Analytics Learning Lab (DALL) and External Reporting teams should be invited to review the new methodology and discussions held on possible alternatives. Where practicable, all statisticians, data scientists, and analysts across the NHSBSA should be using standardised methods for analysis.\nWhere a new method is statistically innovative or a wholesale change from what has been used before it may be advised to consult the Methodology Advisory Service(MAS) at the Office for National Statistics (ONS)."
  },
  {
    "objectID": "guidance/reviewing-content/index.html#user-engagement-and-pre-announcing-changes",
    "href": "guidance/reviewing-content/index.html#user-engagement-and-pre-announcing-changes",
    "title": "Reviewing content",
    "section": "User engagement and pre-announcing changes",
    "text": "User engagement and pre-announcing changes\nOnce a review of content has been completed all changes that have been decided to be carried forward for the new release should be documented in full, including additions/removal of content. The impacts of any changes should also be explored and documented, for example, if a methodological change is being made or if a data source is changing. The views of the NHSBSA Lead Official for Statistics and/or Statistical Publication Lead should then be sought on the proposed changes to provide guidance.\nAny changes that we are making to the statistics must be pre-announced on the NHSBSA website well in advance of the release of the statistics (at least 4 weeks in advance). Where practicable, users views should be sought on the proposed changes, but were these changes have been guided by user feedback already this may not be necessary.\nWhere large methodological changes have taken place or where the underlying data source for the statistics has changed and materially impacted new or historical figures users views must be sought before the change is implemented — particularly for National Statistics publications. User engagement can be done through a variety of methods, including:\n\ndedicated engagement events\npublic consultations\nblogs detailing the changes and impacts\nother activities\n\nUser engagement is often done in conjunction with other teams throughout the organisation such as Communications and Customer Insight. These teams should be consulted to help support our engagement activities."
  },
  {
    "objectID": "guidance/reviewing-content/index.html#checklist",
    "href": "guidance/reviewing-content/index.html#checklist",
    "title": "Reviewing content",
    "section": "checklist",
    "text": "checklist\n\ndo you know what analyses are in the statistics and their data source?\nare all data sources still available?\nwhat policy developments have their been?\nhave there been any FOI/data/PQ requests that show there is a gap in the statistics?\nare new services/areas for analysis available? Have any services stopped?\nhave users provided any feedback that can be acted upon?\nhave there been any data quality incidents that would impact the statistics?\nwhat areas did we identify for improvement from the previous release’s debrief?\nhave you documented the review process and formulated an improvement plan for the release"
  },
  {
    "objectID": "diagrams/option2.html",
    "href": "diagrams/option2.html",
    "title": "test2",
    "section": "",
    "text": "Issues:\n\nGraph is too small\nFigure out how to link to tabs with information\nConfigure NHSBSA Theme for diagram?\n\nNotes:\n\nChange theme to base to edit format and colours below\n\n\n\n\n\n%%{\n  init: {\n    'theme': 'neutral',\n    'themeVariables': {\n      'primaryColor': '#BB2528',\n      'primaryTextColor': '#fff',\n      'primaryBorderColor': '#7C0000',\n      'lineColor': '#F8B229',\n      'secondaryColor': '#006100',\n      'tertiaryColor': '#fff',\n      'fontSize': '16px'\n    }\n  }\n}%%\n\nflowchart TB\n  1A[\"Problem Identified\"] \n  1B[\"Assign Critical Friend\"]\n  1C[\"Initial Research and Look at Previous Iterations\"]\n  1D[\"Initial Meeting with Customer\"]\n  1E[\"Get Access to Data\"]\n  1F[\"Create JIRA, SharePoint, Git and Others\"]\n    \n  1G[\"Write Problem \n    Definition\"] \n  1H[\"Data and \n    Business \n    Understanding\"] \n  1I[\"Further \n    Research\"] \n  1J[\"Meet with \n    Customer\"]\n  \n  2A[\"Data Preparation\"]\n  2B[\"Analysis/Modelling\"] \n  2C[\"Reporting and Visuals\"] \n  2D[\"Review and Test\"] \n  2E[\"Critical Friend to Review\"] \n  2F[\"Customer Review\"]\n  \n  3A[\"Final Meeting with Customer\"]\n  3B[\"Wrap Up and  Clean Code\"]\n  3C[\"Write Documentation\"]\n  3D[\"Add to Wiki\"]\n  3E[\"Make Repo Public If Applicable\"]\n  3F[\"Document Any Further Work That Can Be Done\"]\n  \n  subgraph first_two[\" \"]\n  direction LR\n\n    subgraph pre_pd[\"01: Project Setup\"]\n    direction TB\n      1A --&gt; 1B\n      1B --&gt; 1C\n      1C --&gt; 1D\n      1D --&gt; 1E\n      1E --&gt; 1F\n    end\n  \n    subgraph pd_loop[\"02: Problem Definition\"]\n    direction TB\n      1G --&gt; 1H --&gt; 1I --&gt; 1J --&gt; 1G\n    end\n    \n  end\n  \n  subgraph second_two[\" \"]\n  direction LR\n  \n    subgraph analysis_loop[\"03: Analysis/Modelling\"]\n    direction TB\n      2A --&gt; 2B --&gt; 2C --&gt; 2D --&gt; 2E --&gt; 2F --&gt; 2A\n    end\n  \n    subgraph review[\"04: Initiative Review\"]\n    direction TB\n      3A --&gt; 3B\n      3B --&gt; 3C\n      3C --&gt; 3D\n      3D --&gt; 3E\n      3E --&gt; 3F\n    end\n  \n  end\n\n   pre_pd --&gt;|Items &lt;BR&gt; Created| pd_loop\n  analysis_loop --&gt;|Work &lt;BR&gt; Completed| review\n  first_two--&gt;|Sign Off From Customer| second_two\n\n  \n  classDef padding stroke:none,fill:none\n  \n  class first_two padding\n  class second_two padding\n  \n  click 1B \"http://www.github.com\"\n\n\n\n\n\npre_pd –&gt;|Items  Created| pd_loop pd_loop –&gt;|Sign Off From  Customer| analysis_loop analysis_loop –&gt;|Work  Completed| review\nclassDef padding stroke:none,fill:none\nclassDef title font-size:26px classDef pad padding-left:5em;\nclass review title class review pad"
  },
  {
    "objectID": "diagrams/option3.html",
    "href": "diagrams/option3.html",
    "title": "test2",
    "section": "",
    "text": "Issues:\n\nGraph is too small\nFigure out how to link to tabs with information\nConfigure NHSBSA Theme for diagram?\n\nNotes:\n\nChange theme to base to edit format and colours below\n\n\n\n\n\n%%{\n  init: {\n    'theme': 'neutral',\n    'themeVariables': {\n      'primaryColor': '#BB2528',\n      'primaryTextColor': '#fff',\n      'primaryBorderColor': '#7C0000',\n      'lineColor': '#F8B229',\n      'secondaryColor': '#006100',\n      'tertiaryColor': '#fff',\n      'fontSize': '16px'\n    }\n  }\n}%%\n\nflowchart LR\n  1A[\"Problem Identified\"] \n  1B[\"Assign Critical Friend\"]\n  1C[\"Initial Research and Look at Previous Iterations\"]\n  1D[\"Initial Meeting with Customer\"]\n  1E[\"Get Access to Data\"]\n  1F[\"Create JIRA, SharePoint, Git and Others\"]\n    \n  1G[\"Write Problem \n    Definition\"] \n  1H[\"Data and \n    Business \n    Understanding\"] \n  1I[\"Further \n    Research\"] \n  1J[\"Meet with \n    Customer\"]\n  \n  2A[\"Data Preparation\"]\n  2B[\"Analysis/Modelling\"] \n  2C[\"Reporting and Visuals\"] \n  2D[\"Review and Test\"] \n  2E[\"Critical Friend to Review\"] \n  2F[\"Customer Review\"]\n  \n  3A[\"Final Meeting with Customer\"]\n  3B[\"Wrap Up and  Clean Code\"]\n  3C[\"Write Documentation\"]\n  3D[\"Add to Wiki\"]\n  3E[\"Make Repo Public If Applicable\"]\n  3F[\"Document Any Further Work That Can Be Done\"]\n  \n  subgraph first_two[\" \"]\n  direction TB\n\n    subgraph pre_pd[\"01: Project Setup\"]\n    direction TB\n      1A --&gt; 1B\n      1B --&gt; 1C\n      1C --&gt; 1D\n      1D --&gt; 1E\n      1E --&gt; 1F\n    end\n  \n    subgraph pd_loop[\"02: Problem Definition\"]\n    direction TB\n      1G --&gt; 1H --&gt; 1I --&gt; 1J --&gt; 1G\n    end\n    \n  end\n  \n  subgraph second_two[\" \"]\n  direction TB\n  \n    subgraph analysis_loop[\"03: Analysis/Modelling\"]\n    direction TB\n      2A --&gt; 2B --&gt; 2C --&gt; 2D --&gt; 2E --&gt; 2F --&gt; 2A\n    end\n  \n    subgraph review[\"04: Initiative Review\"]\n    direction TB\n      3A --&gt; 3B\n      3B --&gt; 3C\n      3C --&gt; 3D\n      3D --&gt; 3E\n      3E --&gt; 3F\n    end\n  \n  end\n\n  pre_pd --&gt;|Items &lt;BR&gt; Created| pd_loop\n  analysis_loop --&gt;|Work &lt;BR&gt; Completed| review\n  first_two --- second_two\n  \n  linkStyle 22 stroke:white\n\n  \n  classDef padding stroke:none,fill:none\n  \n  class first_two padding\n  class second_two padding\n  \n  click 1B \"http://www.github.com\"\n\n\n\n\n\npre_pd –&gt;|Items  Created| pd_loop pd_loop –&gt;|Sign Off From  Customer| analysis_loop analysis_loop –&gt;|Work  Completed| review\nclassDef padding stroke:none,fill:none\nclassDef title font-size:26px classDef pad padding-left:5em;\nclass review title class review pad"
  },
  {
    "objectID": "guidance/analysis/index.html",
    "href": "guidance/analysis/index.html",
    "title": "Analysis",
    "section": "",
    "text": "G\n\n \n\ncluster_analysis_loop\n\n       03: Analysis and modelling  \n\ncluster_dummy_c\n\n   \n\nc1\n\n       Data preparation   \n\nc2\n\n       Analysis and modelling   \n\nc1-&gt;c2\n\n    \n\nc3\n\n       Output artifacts   \n\nc2-&gt;c3\n\n    \n\nc4\n\n       Team review   \n\nc3-&gt;c4\n\n    \n\nc5\n\n       Critical friend review   \n\nc4-&gt;c5\n\n    \n\nc6\n\n       Customer review   \n\nc5-&gt;c6\n\n    \n\nc6-&gt;c1"
  },
  {
    "objectID": "guidance/analysis/index.html#data-preparation",
    "href": "guidance/analysis/index.html#data-preparation",
    "title": "Analysis",
    "section": "Data preparation",
    "text": "Data preparation\nThe goal of this step is to end up with one or more base tables on which analysis is then conducted. The base tables should include any data relevant, or possibly relevant, to the initiative. Data preparation may be iterative, as the analysis becomes refined with frequent feedback from stakeholders."
  },
  {
    "objectID": "guidance/analysis/index.html#analysis-and-modelling",
    "href": "guidance/analysis/index.html#analysis-and-modelling",
    "title": "Analysis",
    "section": "Analysis and modelling",
    "text": "Analysis and modelling\nIn this step we turn data into insights. This could involve a plethora of data science techniques from statistics to machine learning. The choice of which techniques to use will depend on the desired outputs of the initiative and what the data available allows. The problem definition should be used to steer the analysis towards delivery of the agreed outputs."
  },
  {
    "objectID": "guidance/analysis/index.html#output-artifacts",
    "href": "guidance/analysis/index.html#output-artifacts",
    "title": "Analysis",
    "section": "Output artifacts",
    "text": "Output artifacts\nThe expected outputs of an initiative will have been agreed in the problem definition. These will range from written reports, summary slide decks, data sets and applications such as dashboards or interactive reports. Getting frequent feedback is important, as it helps to prevent drift from the problem definition and allows more people to spot problems as early as possible. ?There are general guidelines that apply across most types of output."
  },
  {
    "objectID": "guidance/analysis/index.html#team-review",
    "href": "guidance/analysis/index.html#team-review",
    "title": "Analysis",
    "section": "Team review",
    "text": "Team review\nFrequent reviews of the work give a chance to pick up on any issues before asking third parties to look at it. An important component of this is code review. With the mindset that the code may become public, put yourself in the shoes of someone coming across the code and wanting to both understand it and then reuse it. This means keeping in mind the principles of Reproducible Analytical Pipelines (RAP), which benefits others coming across the code in future, but also the team themselves while continuing to work on it."
  },
  {
    "objectID": "guidance/analysis/index.html#critical-friend-review",
    "href": "guidance/analysis/index.html#critical-friend-review",
    "title": "Analysis",
    "section": "Critical friend review",
    "text": "Critical friend review\nThe critical friend is an integral part of the team. They serve as someone to bounce ideas off, offer advice and act as another pair of eyes and ears that may pick up up on something unnoticed by the rest of the team. So keep them in the loop as much as possible by asking them to review things before taking anything to the customer of the initiative."
  },
  {
    "objectID": "guidance/analysis/index.html#customer-review",
    "href": "guidance/analysis/index.html#customer-review",
    "title": "Analysis",
    "section": "Customer review",
    "text": "Customer review\nBefore asking the customer to review and feedback on the work, ensure that whichever output they are looking at is either complete according to the problem definition, or for work in progress that you have made it clear what points on the problem definition are (or are not) being covered so far. Use the feedback to help steer things back on course if necessary. Something that can occur is the customer now asking for something different, or entirely new. This should be handled with care, as over-committing can backfire. In such cases it may be better to propose a future phase of work for any additional outputs requested."
  },
  {
    "objectID": "guidance/analysis/analysis-steps/01-data-prep.html",
    "href": "guidance/analysis/analysis-steps/01-data-prep.html",
    "title": "Data preparation",
    "section": "",
    "text": "The goal of this step is to end up with one or more base tables on which analysis is then conducted. The base tables should include any data relevant, or possibly relevant, to the initiative. Data preparation may be iterative, as the analysis becomes refined with frequent feedback from stakeholders.\nThe tables relevant to the initiative will have been identified in the process of writing the problem definition. Before using them it is important to get an idea of the data quality. This consists of summary statistics including\n\nData type\nMost common values for categorical fields, or minima/maxima and median for numeric\nCompleteness\n\n\n\nThis is the data type as stored. This may or may not be the same as what you expect. For example, it is common to use a numeric key in a fact table that represents a categorical value found in a dimension table.\n\n\n\nFor categorical fields, it is useful to have an idea of the distribution of values. For example, if considering use of machine learning techniques that are sensitive to imbalanced data. Even when considering techniques that handle imbalanced data well keeping the distribution in mind is useful when interpreting and reporting on the the results.\n\n\n\nFor numeric fields, this is the analogue to common values for categorical data. The values will give you an idea of how the data is distributed. You could go further and plot a histogram for better visualisation of this. NOTE: Consider adding to Rmarkdown example\n\n\n\nMany techniques are sensitive to missing data. Extreme cases of this can even prevent any useful analysis, so needs to be mitigated in some way. How it is mitigated will depend on the techniques employed. One thing to look out for is cases where a field has values only after, or before, a certain time. When this occurs it may limit some specific analysis to the time period for which values exist.\n\n\n\nThe template initiative Data folder contains a Rmarkdown, data_summary.Rmd, which can be used as an example or starting point for data quality checking. When using it make sure to add notes of the findings for future reference.\nAn alternative would be to do it directly in the database, such as suggested on this github issue.\nIn addition to any Rmarkdown used, you may find it helpful to keep a record of specific counts found during initial EDA and during any further EDA performed while preparing the base tables. An example Excel file is in the template initiative Data folder, EDA figures.xlsx."
  },
  {
    "objectID": "guidance/analysis/analysis-steps/01-data-prep.html#data-quality",
    "href": "guidance/analysis/analysis-steps/01-data-prep.html#data-quality",
    "title": "Data preparation",
    "section": "",
    "text": "The goal of this step is to end up with one or more base tables on which analysis is then conducted. The base tables should include any data relevant, or possibly relevant, to the initiative. Data preparation may be iterative, as the analysis becomes refined with frequent feedback from stakeholders.\nThe tables relevant to the initiative will have been identified in the process of writing the problem definition. Before using them it is important to get an idea of the data quality. This consists of summary statistics including\n\nData type\nMost common values for categorical fields, or minima/maxima and median for numeric\nCompleteness\n\n\n\nThis is the data type as stored. This may or may not be the same as what you expect. For example, it is common to use a numeric key in a fact table that represents a categorical value found in a dimension table.\n\n\n\nFor categorical fields, it is useful to have an idea of the distribution of values. For example, if considering use of machine learning techniques that are sensitive to imbalanced data. Even when considering techniques that handle imbalanced data well keeping the distribution in mind is useful when interpreting and reporting on the the results.\n\n\n\nFor numeric fields, this is the analogue to common values for categorical data. The values will give you an idea of how the data is distributed. You could go further and plot a histogram for better visualisation of this. NOTE: Consider adding to Rmarkdown example\n\n\n\nMany techniques are sensitive to missing data. Extreme cases of this can even prevent any useful analysis, so needs to be mitigated in some way. How it is mitigated will depend on the techniques employed. One thing to look out for is cases where a field has values only after, or before, a certain time. When this occurs it may limit some specific analysis to the time period for which values exist.\n\n\n\nThe template initiative Data folder contains a Rmarkdown, data_summary.Rmd, which can be used as an example or starting point for data quality checking. When using it make sure to add notes of the findings for future reference.\nAn alternative would be to do it directly in the database, such as suggested on this github issue.\nIn addition to any Rmarkdown used, you may find it helpful to keep a record of specific counts found during initial EDA and during any further EDA performed while preparing the base tables. An example Excel file is in the template initiative Data folder, EDA figures.xlsx."
  },
  {
    "objectID": "guidance/analysis/analysis-steps/01-data-prep.html#data-cleansing",
    "href": "guidance/analysis/analysis-steps/01-data-prep.html#data-cleansing",
    "title": "Data preparation",
    "section": "Data cleansing",
    "text": "Data cleansing\n\nMissingness\nThe data may have some degree of missingness. How it is handled will depend on the both the data and the techniques to be employed in the analysis. There are many techniques to handle this. Sometimes the best method is obvious, sometimes you will have to choose between several potentially viable methods.\n\n\nData types\nFrom the data summary generated you may find some fields that are better expressed in a different data type.\n\n\nKeep or discard fields\nIn the first iteration it is best to keep any and all fields that may be useful in the analysis. As the analysis is refined through exploration and feedback, you can discard fields no longer deemed relevant. By the end, only fields which are used in the analysis should remain in the base tables. An exception to this is when a future phase of work is planned. In such cases it may make sense to keep some fields not immediately useful available."
  },
  {
    "objectID": "guidance/analysis/analysis-steps/01-data-prep.html#rap-considerations",
    "href": "guidance/analysis/analysis-steps/01-data-prep.html#rap-considerations",
    "title": "Data preparation",
    "section": "RAP considerations",
    "text": "RAP considerations\nDon’t repeat yourself is a well known adage in software development. While we are not writing software, it still applies and is a core principle of RAP. Pull out repeated code into functions; this allows for good tests to be written, another core principle of RAP. Variables should not be spread throughout a script, but instead gathered together into a config file or single block at the top of a script."
  },
  {
    "objectID": "guidance/analysis/analysis-steps/01-data-prep.html#note-keeping",
    "href": "guidance/analysis/analysis-steps/01-data-prep.html#note-keeping",
    "title": "Data preparation",
    "section": "Note keeping",
    "text": "Note keeping\nThroughout data preparation, ensure to make notes (saved in the initiative folder) of any potential or realised issues, caveats to be included in final outputs and decisions made with their reasoning. This will help in writing the supporting text in the main outputs."
  },
  {
    "objectID": "guidance/analysis/analysis-steps/01-data-prep.html#validation",
    "href": "guidance/analysis/analysis-steps/01-data-prep.html#validation",
    "title": "Data preparation",
    "section": "Validation",
    "text": "Validation\nSome thought should be given to how to validate the base tables are a true reflection of what is intended. Some data may be checked against other data sources, such ePACT2. In addition, adding tests is a worthwhile investment. With tests you can be more confident that any future change that introduces an error will be caught quickly.\nIf the data is something that you would expect personal details to appear in, you can also confirm you see what you would expect for yourself."
  },
  {
    "objectID": "guidance/analysis/analysis-steps/01-data-prep.html#miro-points-for-reference-to-be-removed",
    "href": "guidance/analysis/analysis-steps/01-data-prep.html#miro-points-for-reference-to-be-removed",
    "title": "Data preparation",
    "section": "Miro points (for reference, to be removed)",
    "text": "Miro points (for reference, to be removed)\nKeep small spreadsheet to keep track of all EDA results\nAlso links with business understanding\nInvest the time into creating accurate base tables\nEither R or SQL for her analysis, just whatever she feels like\nR markdown is quite good to lift into presentations\nUses data miner to extract insights and aggregate SQL tables with the info\nDALP data is mostly snapshot data, some get automatically refreshed, otherwise we ask for it, get it from DWCP or external system\nCan do SQL script and put all results into Excel\nSQL to do base tables, get all the columns you need and cleaned. Then analysis in R\nCould pull base table into R via DBI or DBPLYR\nKeep detailed notes as you progress - Excel, Rmd, PowerPoint. This will speed up creating your outputs"
  },
  {
    "objectID": "guidance/analysis/analysis-steps/02-analysis-modelling.html",
    "href": "guidance/analysis/analysis-steps/02-analysis-modelling.html",
    "title": "Analysis and modelling",
    "section": "",
    "text": "In this step we turn data into insights. This could involve a plethora of data science techniques from statistics to machine learning. The choice of which techniques to use will depend on the desired outputs of the initiative and what the data available allows. The problem definition should be used to steer the analysis towards delivery of the agreed outputs.\nNOTE: What to write for this? There is no way to cover all possible techniques, and other than the below points that also applied to the data prep, I am finding it hard to think of anything else that would apply in general…"
  },
  {
    "objectID": "guidance/analysis/analysis-steps/02-analysis-modelling.html#rap-considerations",
    "href": "guidance/analysis/analysis-steps/02-analysis-modelling.html#rap-considerations",
    "title": "Analysis and modelling",
    "section": "RAP considerations",
    "text": "RAP considerations\nDon’t repeat yourself is a well known adage in software development. While we are not writing software, it still applies and is a core principle of RAP. Pull out repeated code into functions; this allows for good tests to be written, another core principle of RAP. Variables should not be spread throughout a script, but instead gathered together into a config file or single block at the top of a script."
  },
  {
    "objectID": "guidance/analysis/analysis-steps/02-analysis-modelling.html#note-keeping",
    "href": "guidance/analysis/analysis-steps/02-analysis-modelling.html#note-keeping",
    "title": "Analysis and modelling",
    "section": "Note keeping",
    "text": "Note keeping\nThroughout the analysis, ensure to make notes (saved in the initiative folder) of any potential or realised issues, caveats to be included in final outputs and decisions made with their reasoning. This will help in writing the supporting text in the main outputs."
  },
  {
    "objectID": "guidance/analysis/analysis-steps/02-analysis-modelling.html#validation",
    "href": "guidance/analysis/analysis-steps/02-analysis-modelling.html#validation",
    "title": "Analysis and modelling",
    "section": "Validation",
    "text": "Validation\nSome thought should be given to how to validate the results of the analysis. Some data may be checked against other data sources, such ePACT2. In addition, adding tests is a worthwhile investment. With tests you can be more confident that any future change that introduces an error will be caught quickly.\nIf the data is something that you would expect personal details to appear in, you can also confirm you see what you would expect for yourself."
  },
  {
    "objectID": "guidance/analysis/analysis-steps/03-output_artifacts.html",
    "href": "guidance/analysis/analysis-steps/03-output_artifacts.html",
    "title": "Output artifacts",
    "section": "",
    "text": "The expected outputs of an initiative will have been agreed in the problem definition. These will range from written reports, summary slide decks, data sets and applications such as dashboards or interactive reports. Getting frequent feedback is important, as it helps to prevent drift from the problem definition and allows more people to spot problems as early as possible."
  },
  {
    "objectID": "guidance/analysis/analysis-steps/03-output_artifacts.html#supporting-text",
    "href": "guidance/analysis/analysis-steps/03-output_artifacts.html#supporting-text",
    "title": "Output artifacts",
    "section": "Supporting text",
    "text": "Supporting text\nA written report could be be done in Word or markdown (plain or Rmarkdown, depending on need to run and include code and code output). Using markdown the output could be rendered as either Word or PDF. Alternatively, the output may be in the form of an app, such as a Quarto site, Rmarkdown HTML output or Shiny dashboard. Some style guidelines applicable to all these outputs include\n\nKey findings at the front as people won’t read the whole thing\nInclude caveats and assumptions, and for particularly important ones mention them up front\nNumbers under 10 are in words, over 10 in digits (but use common sense!)\nUse bold tag lines for emphasis of important points\nSometimes difficult to do, but try to have a story\nIf appropriate, include recommendations and ideas for further exploration\nSentences shorter than 20 words\nWrite it so that a 12 year old would be able to understand it\n\nFor a sense of cohesiveness, have one person be the editor. This does not mean they must write everything themselves from scratch. Often when working as a team individuals will be responsible for different parts of the whole piece. They should write up their own parts, which can then be re-written by the person taking the editor role. The whole team can then review and agree on a final version that speaks with one clear voice.\nA very useful tool for written output is Hemingway Editor. If you can get your text to mostly have no issues in this, you know it is readable.\nIt is advised to leave writing any text supporting visuals such as charts or tables until the team is almost certain they are finalised."
  },
  {
    "objectID": "guidance/analysis/analysis-steps/03-output_artifacts.html#slide-decks",
    "href": "guidance/analysis/analysis-steps/03-output_artifacts.html#slide-decks",
    "title": "Output artifacts",
    "section": "Slide decks",
    "text": "Slide decks\nAll initiatives should have a slide deck as an agreed output. At a minimum this should be detailed enough to state the problems or questions the initiative looked at, the approach used to address or answer these and a summary of the most important findings. Aim to have most of the content be visual, with limited amounts of explanatory text.\nNOTE: Tom and/or Ryan may have more to add to this as I missed the session covering slide deck creation."
  },
  {
    "objectID": "guidance/analysis/analysis-steps/03-output_artifacts.html#applications",
    "href": "guidance/analysis/analysis-steps/03-output_artifacts.html#applications",
    "title": "Output artifacts",
    "section": "Applications",
    "text": "Applications\nShiny powered applications should use the template provided by the nhsbsaShinyR repository. This repo is continually being refined, so will include code already tested (including accessibility testing). Using the same starting point also means our Shiny outputs will have a single easily recognisable look and feel.\nThe other platform we use to deliver output as an application is Power BI. This platform is much newer, so best working practices are still being discovered. Similarly to the nhsbsaShinyR template for Shiny, there is a template for use in Power BI.\nNOTE: Thomas may have more to add to this as he is more familiar with Power BI.\n\nRAP and development considerations\nDon’t repeat yourself is a well known adage in software development. While we are not writing software, it still applies and is a core principle of RAP. Pull out repeated code into functions; this allows for good tests to be written, another core principle of RAP. Variables should not be spread throughout a script, but instead gathered together into a config file or single block at the top of a script.\nThe nhsbsaShinyR template for Shiny is a good starting point for creating the app with RAP in mind. An excellent book on the subject of Shiny apps is Engineering Production-Grade Shiny Apps. Whilst not written specifically with RAP in mind, many of the principles and techniques are relevant. Following the advice and techniques in this book should produce a very professional and well-planned app. The authors are the team behind the golem framework, ThinkR, used in the nhsbsaShinyR template.\nFor a more in-depth look at what can be done in Shiny, there is Outstanding User Interfaces with Shiny. This is particularly helpful when trying to address an accessibility failing of some content.\n\n\nIssue tracking\nIt is recommended to use GitHub issues for tracking essential and potential changes or fixes. Issues allow easy creation of branches and pull requests, keeping the non-coding tasks all in one place.\n\n\nTesting and code reviews\nWhen some set of changes or fixes is ready, make sure to run the whole app and go through any content related to the change. Add tests as you add functionality, you will thank yourself later! When code reviewing, always download and run the app. This catches issues related to something being present in someones environment, which is no longer there when someone else runs it."
  },
  {
    "objectID": "guidance/analysis/analysis-steps/03-output_artifacts.html#data-set-outputs",
    "href": "guidance/analysis/analysis-steps/03-output_artifacts.html#data-set-outputs",
    "title": "Output artifacts",
    "section": "Data set outputs",
    "text": "Data set outputs\nSometimes the customer requires a regularly updated data set to be provided as an output. Done with little thought, this can result in unnecessary overhead that continues indefinitely. It is recommended to productionise the process as much as it can be, in line with the principles of RAP."
  },
  {
    "objectID": "guidance/analysis/analysis-steps/03-output_artifacts.html#miro-points-for-reference-to-be-removed",
    "href": "guidance/analysis/analysis-steps/03-output_artifacts.html#miro-points-for-reference-to-be-removed",
    "title": "Output artifacts",
    "section": "Miro points (for reference, to be removed)",
    "text": "Miro points (for reference, to be removed)\nKey findings at the front as people won’t read the whole thing\nAdd in caveats and assumptions\nNumbers under 10 are in words, over 10 in digits\nBold tag lines are appreciated\nAlways need to have a story\nRecommendation section to what we could do next, when do they want us to do it?\nSentences shorter than 20 words!!!\nExplain everything in laymen’s terms, as if they don’t know anything\nEither just a presentation or speadsheet\nSlides for it too to present (small as possible)\nSend them it ahead of time to read\nEither small presentation to stakeholders and mini handover\nDo webinars to spread awareness\nConferences\nSummary slides\nPut in text AFTER all the graphs are happy with, to avoid duplication\nSelf assigned projects, present to Nadine?\nWouldn’t write it with Nadine in mind but would make suggestions\nBetter for one person to writeup the results or report, with others providing peer review. One voice comes across better than multiple writing styles."
  },
  {
    "objectID": "guidance/analysis/analysis-steps/04-team-review-test.html",
    "href": "guidance/analysis/analysis-steps/04-team-review-test.html",
    "title": "Team review",
    "section": "",
    "text": "Frequent reviews of the work give a chance to pick up on any issues before asking third parties to look at it. An important component of this is code review. With the mindset that the code may become public, put yourself in the shoes of someone coming across the code and wanting to both understand it and then reuse it. This means keeping in mind the principles of Reproducible Analytical Pipelines (RAP), which benefits others coming across the code in future, but also the team themselves while continuing to work on it.\nThe basic idea of the analysis loop is a tight cycle of refine and get feedback. Before asking for feedback, the team needs to ensure that they have checked everything is as they expect it to be. This includes code and any supporting text or other artifacts such as data sets, slide decks etc. By doing the analysis in a reproducible way, using RAP principles, this continuous improvement can be made to run smoothly."
  },
  {
    "objectID": "guidance/analysis/analysis-steps/04-team-review-test.html#is-the-code-in-a-good-state",
    "href": "guidance/analysis/analysis-steps/04-team-review-test.html#is-the-code-in-a-good-state",
    "title": "Team review",
    "section": "Is the code in a good state?",
    "text": "Is the code in a good state?\n\nTesting\nConsider using a test driven development (TDD) approach to writing functions. This has many benefits but comes with the drawback of investing time up-front, with no guarantee that it will be useful. TDD means knowing immediately when something is wrong and where the bug occurs. In the long run, the time saved will outweigh the up-front investment.\nEven if not using TDD, you should be testing code as you develop it. Since you are testing, spend a bit longer on making these tests reproducible automatically.\nFor R, it is recommended to use the testthat package.\nFor python, use of ?? is recommended.\n\n\nComments\nIn general, the purpose of code should be apparent to someone who is familiar with the language. If it is not, first try to find a more readable and clear way to write it. If not, code comments should be present to explain it. Also, you can split code into sections. For example in Rstudio by using Ctrl+Shift+R.\n\n\nDocumentation\nPeople other than the authors may look at and use our code. This could be other members of the team, colleagues in the NHSBSA or the public. Our code should be well documented. This is both professional and increases ease of reuse and maintenance.\nFor R, it is recommended to use the roxygen2 package. See chapter 16 of R Packages (2e) for details by the author of roxygen2.\nFor python, use of ?? is recommended."
  },
  {
    "objectID": "guidance/analysis/analysis-steps/04-team-review-test.html#textual-content",
    "href": "guidance/analysis/analysis-steps/04-team-review-test.html#textual-content",
    "title": "Team review",
    "section": "Textual content",
    "text": "Textual content\nA common component of the outputs we create is text. Be aware that the refine-feedback cycle can be challenging if you have such text in multiple places. For example, in a Word document used for review, but also in a shiny dashboard. Try to keep all text in a single place until as late as possible in the initiative life cycle.\nSpecifically for R, there is text review functionality built-in to the nhsbsaShinyR template. If you have this installed, see the vignette on this by running vignette(\"Text review\", \"nhsbsaShinyR\"). This allows for using a Word document initially, the contents of which can be automatically used to create markdown content for the app. It works the other way also, to provide a Word document constructed from existing markdown files. This makes syncing the text when it has changed due to review feedback back into the app a mostly automatic process."
  },
  {
    "objectID": "guidance/analysis/analysis-steps/04-team-review-test.html#accessibility-testing",
    "href": "guidance/analysis/analysis-steps/04-team-review-test.html#accessibility-testing",
    "title": "Team review",
    "section": "Accessibility testing",
    "text": "Accessibility testing\nIf your output is to be available online, it must be tested for accessibility. Full details are on the DALL wiki.\nBear in mind that testing accessibility too early can result in wasted time, as aspects of the app can and do change. So in general, leave such testing until fairly confident that only minor adjustments and text agreement remain to be done."
  },
  {
    "objectID": "guidance/analysis/analysis-steps/04-team-review-test.html#miro-points-for-reference-to-be-removed",
    "href": "guidance/analysis/analysis-steps/04-team-review-test.html#miro-points-for-reference-to-be-removed",
    "title": "Team review",
    "section": "Miro points (for reference, to be removed)",
    "text": "Miro points (for reference, to be removed)\nTest driven development: Write the tests before the code?\nIf scope changes account for additional time later in the projects\nIdeally do accessibility testing upfront or as you go so you don’t have to change loads of stuff"
  },
  {
    "objectID": "guidance/analysis/analysis-steps/05-critical-friend-review.html",
    "href": "guidance/analysis/analysis-steps/05-critical-friend-review.html",
    "title": "Critical friend review",
    "section": "",
    "text": "The critical friend is an integral part of the team. They serve as someone to bounce ideas off, offer advice and act as another pair of eyes and ears that may pick up up on something unnoticed by the rest of the team. So keep them in the loop as much as possible by asking them to review things before taking anything to the customer of the initiative.\nThe critical friend should review the process, code and outputs of the team. This should be done before asking the initiative customer to review the outputs. The ask is always “be as thorough as you can”, but other commitments can make a thorough review challenging. So, where possible, give the critical friend advance notice that you expect to be ready for their review soon.\nAt a minimum, the critical friend should be"
  },
  {
    "objectID": "guidance/analysis/analysis-steps/05-critical-friend-review.html#miro-points-for-reference-to-be-removed",
    "href": "guidance/analysis/analysis-steps/05-critical-friend-review.html#miro-points-for-reference-to-be-removed",
    "title": "Critical friend review",
    "section": "Miro points (for reference, to be removed)",
    "text": "Miro points (for reference, to be removed)\nIdeally create an issue and PR combination and have them reviewed as you go by the critical friend, rather than a massive check at the end\nCan be an element of waiting around while waiting for review\nCan be make a branch on another branch to continue while waiting\nGet them involved as early as possible\nKeep them in the loop throughout, rather than right at the end\nThey should refer to the PD and make sure everything is correct and on track\nPotentially they should just be on the output and make sure it looks good, any ideas?\nDon’t accept suggestions just to be “nice”. Similarly, be confident in making suggestions and don’t worry about offending people! This is part of the value a critical friend should bring to a project.\nIf reviewing code make sure to give the critical friend small but frequent bits of code to review\nFind a critical friend, who has done the coding before or content or both\nBring the critical friend along for a “second set of ears”\nHard to find critical friend, conscious everyone is busy\nTable or JIRA to see what everyone is working on, if they’re already a critical friend?\nInvolve your critical friends from the beginning - they may notice things you missed\nGive your critical friend notice that stuff to review will be coming their way\nDon’t be afraid to voice your opinion/challenge more senior members of the team"
  },
  {
    "objectID": "guidance/analysis/analysis-steps/06-customer-review.html",
    "href": "guidance/analysis/analysis-steps/06-customer-review.html",
    "title": "Customer review",
    "section": "",
    "text": "Before asking the customer to review and feedback on the work, ensure that whichever output they are looking at is either complete according to the problem definition, or for work in progress that you have made it clear what points on the problem definition are (or are not) being covered so far. Use the feedback to help steer things back on course if necessary. Something that can occur is the customer now asking for something different, or entirely new. This should be handled with care, as over-committing can backfire. In such cases it may be better to propose a future phase of work for any additional outputs requested."
  },
  {
    "objectID": "guidance/analysis/analysis-steps/02-analysis-modelling.html#miro-points-for-reference-to-be-removed",
    "href": "guidance/analysis/analysis-steps/02-analysis-modelling.html#miro-points-for-reference-to-be-removed",
    "title": "Analysis and modelling",
    "section": "Miro points (for reference, to be removed)",
    "text": "Miro points (for reference, to be removed)\nKeep detailed notes as you progress - Excel, Rmd, PowerPoint. This will speed up creating your outputs\nGood tip: Search for yourself as you know it’s correct\nCan use variables in SQL. Also can perform more complex ops with PL/SQL, such as scheduling and loops.\nReplicates a result from Epact2 in SQL as a sanity check\nCreate your own reusable data analysis script/markdown file to make it easier to start some new analysis\nHave a config file to avoid a nightmare later on with changing hard coded variables\nHow best to embed RAP into what we do"
  }
]